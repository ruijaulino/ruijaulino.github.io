# Moving Averages

Computing statistics with recent values is a common practice: predict prices going up or down because they are going up or down recently, estimating model and/or risk parameters on a rolling basis to try to cope with non stationarity, what is the current inflation based on the recent reading and all other cases that we see everyday that apply the concept of estimating with recent observations. 

The objective here is to give a review on how to make sense of this type of measurement. In the end I present a model that can handle a _time-varying_ window to calculate a rolling mean - this means that the effective window that we consider to compute the recent mean changes as the noise parameters that are generating the data change.

## The random walk with noise baseline

A interesting baseline model to understand rolling computations is the random walk with noise: we have some underlying (hidden) process $\mu_t$ and our observations $x_t$ come from some distribution centered at $\mu_t$. This is a simple example of a state space model (in a single dimension) and the solution for the estimation of the hidden variable distribution - filtering - is given by the known Kalman Filter. Both the hidden and observation variable are Gaussian $N(x\|\mu,\sigma^2)=(2\pi)^{-\frac{1}{2}}\sigma^{-2}\exp\left(-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}\right)$.

Based on this, we can describe the system as:

$p(\mu_t\|\mu_{t-1})=N(\mu_t\|\mu_{t-1},q)$


$p(x_t\|z_t)=N(x_t\|\mu_t,\sigma^2)$

where $q$ and $\sigma^2$ are the variances of the state and observation noise, respectivelly. This means that variable $\mu$ performs a random walk and $x_t$ is noise around it.

The following code illustrates the idea.


```python
import numpy as np
import matplotlib.pyplot as plt
q=0.1 # state noise variance
s=1 # observation noise variance
n=100 # number of timestamps
mu=np.zeros(n)
x=np.zeros(n)
for i in range(1,n):
    mu[i]=np.random.normal(mu[i-1],np.sqrt(q))
    x[i]=np.random.normal(mu[i],np.sqrt(s))
plt.plot(mu,label='hidden')
plt.plot(x,label='observation')
plt.grid(True)
plt.legend()
plt.show()
```


    
![png](/images/movavg/output_2_0.png)
    


A quantity of interest is the _filtering_ distribution $p(\mu_t\|\mu_{1:t})$, which means, given the data I saw until now, what is the distribution (to answer what is the most likely value) of $\mu_t$ (we have access to observations $x_t$ only, not on $\mu_t$, which is what we are trying to estimate).

We can write:

$p(\mu_t\|\mu_{1:t}) \propto p(x_t\|\mu_t)p(\mu_t\|x_{1:t-1})$

To solve the problem, we can assume that, at $t-1$, we know the distribution of $\mu_{t-1}$ which will be $p(\mu_{t-1}\|x_{1:t-1})=N(\mu_{t-1}\|m_{t-1},p_{t-1})$ - centered at $m_{t-1}$ with variance $p_{t-1}$.

Now:

$p(\mu_t\|x_{1:t-1}) = \int p(\mu_t\|\mu_{t-1}) p(\mu_{t-1}\|x_{1:t-1}) \text{d}\mu_{t-1} = \int N(\mu_t\|\mu_{t-1},r)N(\mu_{t-1}\|m_{t-1},p_{t-1}) \text{d}\mu_{t-1} $

Which can be solved to (standard gaussian distribution formulas):

$p(\mu_t\|x_{1:t-1}) = N(\mu_t\|m_{t}^-,p_{t}^-)$

with $m_{t}^-=m_{t-1}$ and $p_{t}^-=p_{t-1}+q$. Calculations asside, this basically means that we are adding two normals: one with the uncertainty on where the hidden state is and another with the uncertainty to where it will move.

Then 

$p(\mu_t\|x_{1:t}) \propto N(x_t\|\mu_t,r)N(\mu_t\|m_{t}^-,p_{t}^-) \rightarrow p(\mu_t\|x_{1:t}) = N(\mu_t\|m_t,p_t)$

with 

$m_t^-=m_{t-1}$

$p_t^- = p_{t-1}+q$

$m_t=m_{t}^-+\frac{p_t^-}{p_t^-+\sigma^2}(x_t-m_t^-)$

$p_t = p_t^- - \frac{(p_t^-)^2}{p_t^-+\sigma^2}$

Also, we can notice that the update formula for the mean of the hidden state distribution can be written as

$m_t=\frac{p_t^-}{p_t^-+\sigma^2} m_{t-1}+\frac{p_t^-}{p_t^-+\sigma^2}x_t = \lambda_t x_t + (1-\lambda_t) m_{t-1}$

with $\lambda=\frac{p_t^-}{p_t^-+\sigma^2}$. Also, after many iterations, the values of $p_t$ (and $\lambda_t$) settle to a constant value $p$ (which is the solution of $p=p+q-\frac{\left(p+q\right)^2}{p+q+\sigma^2}$); in practice, the relative weight of a new observation on the mean computation is constant.

This means that, a exponential moving average of the observations is the estimator of the hidden state mean and the relative weight of a new observation compared to the previous estimation is dependent on the noise intensities $q$ and $\sigma^2$.

The following code illustrates the idea applied to the previous example; at each new observation, an estimate of what was the hidden state value that generated the observation is calculated. We can see that it tracks quite well the true hidden state and, for this simple example, it is the optimal solution. One can notice that it is equivalent to a exponential moving average on the observations.


```python
# apply this idea
class SimpleKalman(object):
    def __init__(self,q,s,m=0,p=1):
        self.q=q # state noise covariance
        self.s=s # observation noise covariance
        self.m=m # initial value for hidden state mean estimate
        self.p=p # initial value for hidden state variance estimate
    def update(self,x):
        '''
        x: scalar observarion
        '''
        # prior
        m=self.m
        p=self.p+self.q
        # posterior
        self.m=m+(p/(p+self.s))*(x-m)
        self.p=p-p*p/(p+self.s)
# apply to the previous example
simple_kalman=SimpleKalman(q,s,m=0,p=0.01)
mu_est=np.zeros(n) # store estimates for mean
p_est=np.zeros(n) # store estimates for variance
for i in range(1,n):
    # update model with a new observation
    simple_kalman.update(x[i])
    # store estimate
    mu_est[i]=simple_kalman.m 
    p_est[i]=simple_kalman.p
plt.plot(mu,label='hidden')
plt.plot(mu_est,label='hidden estimate')
plt.plot(x,label='observation')
plt.grid(True)
plt.legend()
plt.show()
plt.plot(p_est,label='hidden state variance')
plt.grid(True)
plt.legend()
plt.show()

```


    
![png](/images/movavg/output_4_0.png)
    



    
![png](/images/movavg/output_4_1.png)
    


#### Practical note
When we compute a moving average of a price it is possible to make the analogy that we are estimating the hidden state mean on a random walk with noise. It makes sense to revert to that level given this is the expected value of the next observation - and we trade in price fluctuations not levels.

Also, one could also track returns (although the means of the hidden state will be very small) and so, when we consider a next fluctuation distribution, this would resemble more a trend following approach: if recent returns are positive we are expecting it to remain positive for the next observation.

Of course there is the practice of computing a moving average on prices and, if the price is above[below] the past mean we say that the _trend_ up[down] up will continue - this is equivalent to compute if the past mean of returns is positive but thinking in terms of the random walk with noise allows one to conciliate these views (because we can also say that, if the price is above the moving average, it will revert to that level).

As a note, consider the the following equivalence.

If we have a sequence of fluctuations $x_0,x_1,\cdots,x_t$ and a corresponding sequence of sum of fluctuations $s_0,s_1,\cdots,s_t$ where $s_t=x_0+x_1+\cdots+x_t = x_t+s_{t-1}$ ($s_t$ can be think as a _log price_ and $x_t$ is the _return_ at $t$).

If we are estimating a moving average of the (log) price with $m_t=\lambda s_t + (1-\lambda)m_{t-1}$ and a moving average of the return with $\mu_t=\lambda x_t + (1-\lambda)\mu_{t-1}$ both at at $t$, then, if we compute $s_t-m_t$ to get a direction of the _trend_ this has the same _sign_, i.e, is proportional to $\mu_t$. Because:

$s_t-m_t = s_t - \lambda s_t - (1-\lambda) m_{t-1} = (1-\lambda)(s_t-m_{t-1}) = (1-\lambda)(x_t+s_{t-1}-m_{t-1})$

by induction (making the analogy with the term $s_{t-1}-m_{t-1}$), we can write:

$s_t-m_t = (1-\lambda)x_t+(1-\lambda)^2x_{t-1}+(1-\lambda)^3x_{t-2}+\cdots$

Now, $\mu_t$ can be written as

$\mu_t = \lambda x_t +(1-\lambda)\mu_{t-1}=\lambda \left( x_t+(1-\lambda)x_{t-1}+(1-\lambda)^2 x_{t-1}+\cdots \right) = \frac{\lambda}{1-\lambda}(s_t-m_t) \propto s_t-m_t$

And so:
- when we use price minus moving average to get a prediction for the fluctuation (trend) we are considering the process that tracks the mean of the fluctuations (and we could just do that).
- when we use moving average minus price to get a prediction for the fluctuation (reversion) we are considering the process that tracks the mean of the prices (and, by analogy, we can track the mean of the fluctuations and revert it).



## Varying observation noise

The previous model provided a way to compute an estimate of the hidden state mean but the noise parameters are constant (or known at each time instant). In finance applications it is common to use rolling estimations of volatilities (standard deviation) as well: for example, using a moving average to track if the asset is going up or down and using a rolling estimation of standard deviation to compute how much are we expecting it to move. Regardless of the application, let us try to build a model where the variance of the observations changes over time ($\sigma^2$ parameter in the previous section) and considering the $q$ parameter fixed.

Again, we are interested in computing the filtering distribution (parameters given all observations until now), now we need a posterior on $\mu$ and $\sigma^2$:

$p(\mu_t,\sigma_t^2\|x_{1:t}) \propto p(x_t\|\mu_t,\sigma_t^2)p(\mu_t,\sigma_t^2\|x_{1:t-1})$

##### NIG prior

Looking at the previous expression, it can make sense to try model the prior $p(\mu_t,\sigma_t^2\|x_{1:t-1})$ as a Normal-Inverse-Gamma distribution (NIG from now on). Let the Inverse-Gamma be one such that $IG(x\|a,b)=\frac{b^a}{\Gamma(a)}x^{-a-1}\exp(-\frac{b}{x})$. To ease the notation, let us drop the time dependencies. Consider the following prior (this is the NIG prior; note the dependence on $\sigma^2$ on the $\mu$ distribution):

$p(\mu,\sigma^2) = N(\mu\|m,\frac{\sigma^2}{k})IG(\sigma^2\|a,b) \propto \frac{1}{\sigma^2}^{\frac{1}{2}+a+1} \exp \left( -\frac{b}{\sigma^2} -\frac{k}{2\sigma^2}\left(\mu-m\right)^2 \right)$

And out data is gaussian (again):

$p(x\|\mu,\sigma^2) = N(x\|\mu,\sigma^2) \propto \frac{1}{\sigma^2}^{\frac{1}{2}} \exp\left( -\frac{1}{2} \frac{(x-\mu)^2}{\sigma^2}\right)$

Now, the distribution of $\mu$ and $\sigma^2$ given that we observed $x$ is:

$p(\mu,\sigma^2\|x) \propto p(x\|\mu,\sigma^2)p(\mu,\sigma^2)$

Replacing the distributions functional forms in the previous expression and some manipuation we can write:

$p(\mu,\sigma^2\|x) \propto  \frac{1}{\sigma^2}^{\frac{1}{2}+a+1+\frac{1}{2}} \exp \left(-\frac{k+1}{2\sigma^2}\left(\mu-\frac{km+x}{k+1}\right)^2 -\frac{1}{\sigma^2}\left( b+\frac{1}{2}\frac{k}{k+1}\left(x-m\right)^2 \right) \right)$

where we can make the association that the posterior, $p(\mu,\sigma^2\|x)$, has the same functional form of the prior, $p(\mu,\sigma^2)$, but with different parameters (they are conjugate):

$p(\mu,\sigma^2\|x) \propto N(\mu\|m',\frac{\sigma^2}{k'})IG(\sigma^2\|a',b')$

with 

$k'=k+1$

$m'=\frac{km+x}{k+1}$

$a'=a+\frac{1}{2}$

$b'=b+\frac{1}{2}\frac{k}{k+1}\left(x-m\right)^2$ 

##### Dynamic model

Back to our dynamical model, $p(\mu_t,\sigma_t^2\|x_{1:t}) \propto p(x_t\|\mu_t,\sigma_t^2)p(\mu_t,\sigma_t^2\|x_{1:t-1})$, we still need to work out some parts of it. Let start with the prior and writting it based on it's dependence on previous parameters values:

$p(\mu_t,\sigma_t^2\|x_{1:t-1})=\int p(\mu_t,\sigma_t^2\|\mu_{t-1},\sigma_{t-1}^2)p(\mu_{t-1},\sigma_{t-1}^2\|x_{1:t-1}) \text{d}\mu_{t-1}\text{d}\sigma_{t-1}^2$

In a similar fashion as we did in the Kalman Filter case, suppose that, at $t-1$, we know the hidden variables distribution and let it have the following form:

$p(\mu_{t-1},\sigma_{t-1}^2\|x_{1:t-1}) = N(\mu\|m_{t-1},\frac{\sigma_{t-1}^2}{k_{t-1}})IG(\sigma_{t-1}^2\|a_{t-1},b_{t-1})$

For mathematical convenience, let us make the heuristic assumption that the transformation $p(\mu_t,\sigma_t^2\|\mu_{t-1},\sigma_{t-1}^2)$ is one such that:

$p(\mu_t,\sigma_t^2\|x_{1:t-1}) = N(\mu\|m_{t}^-,\frac{\sigma_{t}^2}{k_{t}^-})IG(\sigma_{t}^2\|a_{t}^-,b_{t}^-)$

with 

$m_t^-=m{t-1}$

$k_t^- = \phi k_{t-1}$

$a_t^-=\phi(a_{t-1}-1)+1$

$b_t^-=b_{t-1} = \phi b_{t-1}$

with $\phi \in [0,1]$. This transformation basically says that we are preserving expectations but increasing the variance/adding uncertainty: this makes sense as a way to propagate parameters forward. In the end, our prior distribution for mean and variance have the same expected value as we had in the previous step but with a larger variance (variance of mean and variance of variance) and, without a proper model for the dynamics, it kind of makes sense to assume expectations remain the same but our uncertainty increases.


With this transformation and the NIG prior formulas we can write that

$p(\mu_t,\sigma_t^2\|x_{1:t}) = N(\mu\|m_t,\frac{\sigma_t^2}{k_t})IG(\sigma_t^2\|a_t,b_t)$

with

$k_t=k_t^-+1$

$m_t=\frac{k_t^-m_t^-+x}{k_t^-+1}$

$a_t=a_t^-+\frac{1}{2}$

$b_t=b_t^-+\frac{1}{2}\frac{k_t^-}{k_t^-+1}\left(x_t-m_t^-\right)^2$ 

With this heuristic we now have a way to track the mean and the variance in the same model.

##### In the limit

After many iterations, $a_t=\phi(a_{t-1}-1)+1+\frac{1}{2}$ will converge to the solution of

$a_{\infty}=\phi(a_{\infty}-1)+1+\frac{1}{2} \rightarrow a_{\infty}=1+\frac{1}{2(1-\phi)}$

Doing the same for $k_t$, $k_t=\phi k_{t-1} + 1$; this gives:

$k_{\infty}=\phi k_{\infty} +1 \rightarrow k_{\infty}=\frac{1}{(1-\phi)}$

We can just use those values. With this, the mean is updated as:

$m_t=\frac{\phi k_{\infty}}{\phi k_{\infty}+1}m_{t-1} + \frac{1}{\phi k_{\infty}+1}x_t = \phi m_{t-1} + (1-\phi)x_t$

and so, just like the Kalman Filter, it is a exponential moving average of the previous values. For $b$:

$b_t=\phi\left( b_{t-1}+\frac{1}{2}\left(x_t-m_{t-1}\right)^2 \right) = \phi^2 b_{t-2} + \frac{1}{2} \phi^2(x_{t-1}-m_{t-2})^2 + \frac{1}{2}\phi(x_t-m_{t-1})^2$

which tends to

$b_t = \sum_{i=0}^t \frac{1}{2} \phi^{t-i+1} (x_i-m_{i-1})^2$

Letting $s=\mathbb{E}[(x_i-m_{i-1})^2]$ and assuming this as a proxy to constant variance (after the effect of the hidden state has been taken out), knowing that $\sum_{k=1}^{\infty} \phi^k = \frac{\phi}{1-\phi}$, after the update, the expected value of the variance is $\mathbb{E}[\sigma^2] = \frac{b_{\infty}}{a_{\infty}-1} = \phi s$. There is a small bias downward if the variance is constant but again, this is not the case that we are assuming.

In the end the formulas make intuitive sense: they resemble what one would do it asked to compute a rolling mean and variance.


The following code implements the previous idea to the same data; also, a rolling estimate is computed as comparison (they should be similar if the moving average window is comparable to the parameter $\phi$). In the generated data, the observation noise is constant, so it does not make much sense to use a changing noise model like this one - this is just for illustration (that why it does not make sense here to compare if the model is _better_; this should be made on a real application). Note that, there is still the problem in computing an appropriate value for $\phi$ but this can be done with some training data (this is not under discussion here but should be easy to derive; one could also think about putting some diffusion in it to make it change over time).


```python

import numpy as np
import matplotlib.pyplot as plt


class NIGTrack(object):
    def __init__(self,phi,m=0,var_init=1,window_init=20):
        self.phi=phi
        self.a=1+1/(2*(1-self.phi))
        self.m=m
        self.b=var_init*(self.a-1) 
        self.is_init=False
        self.window_init=window_init
    def update(self,x):
        x=np.array(x)
        if isinstance(x,np.ndarray):            
            if not self.is_init:
                self.m=np.mean(x[-min(self.window_init,x.size-1):-1])
                self.b=np.var(x[-min(self.window_init,x.size-1):-1])*(self.a-1)
                self.is_init=True
            x=x[-1]
        self.b=self.phi*(self.b+0.5*np.power(x-self.m,2))
        self.m=self.phi*self.m+(1-self.phi)*x
    @property
    def mean(self):
        return self.m
    @property
    def variance(self):
        return self.b/(self.a-1)
    @property
    def std(self):
        return np.sqrt(self.variance)

class RollingEstimator(object):
    def __init__(self,window=20):
        self.window=window
        self.mean=0
        self.variance=1
        self.std=1
    def update(self,x):
        if x.size>self.window:
            self.mean=np.mean(x[-self.window:])
            self.variance=np.var(x[-self.window:])
            self.std=np.std(x[-self.window:])
    
phi=0.8
window=10
nig=NIGTrack(phi)    
roll=RollingEstimator(window)

mean_nig=np.zeros(n)
mean_roll=np.zeros(n)

std_nig=np.zeros(n)
std_roll=np.zeros(n)

for i in range(window,n):
    nig.update(x[:i+1])
    roll.update(x[:i+1])
    mean_nig[i]=nig.mean
    mean_roll[i]=roll.mean
    std_nig[i]=nig.std
    std_roll[i]=roll.std

fig, (ax1,ax2) = plt.subplots(2, sharex=True)
fig.set_size_inches(18.5, 15.5)

ax1.set_title('Mean')
ax1.plot(x[window:],label='Data')
ax1.plot(mean_nig[window:],label='NIG Estimator')
ax1.plot(mean_roll[window:],label='Rolling Estimator')
ax1.legend()
ax1.grid(True)

ax2.set_title('Standard deviation')
ax2.plot(std_nig[window:],label='NIG Estimator')
ax2.plot(std_roll[window:],label='Rolling Estimator')
ax2.legend()
ax2.grid(True)
plt.show()
```


    
![png](/images/movavg/output_9_0.png)
    


## Varying state and observation noise

In both previous models the update formula for $\mu$ is always the same: some value $\lambda$ times the current observation plus $1-\lambda$ times the previous estimation - in other words, the weight given to new observations is always the same. It makes intuitive sense to consider that the _effective_ window (i.e, how many past observations we use) to change if the noise properties are changing. This is the objective of this model. Now, it will be assumed that the state noise $q$ varies in time in a independent way from $\sigma^2$ (different from the prevous model). 

Just like the previous case, we are interested in computing the filtering distribution:

$p(\mu_t,\sigma_t^2\|x_{1:t}) \propto p(x_t\|\mu_t,\sigma_t^2)p(\mu_t,\sigma_t^2\|x_{1:t-1})$

##### Non-Conjugate NIG prior

In this model we want to consider a different type of prior where the noise in $\mu$ is independent of $\sigma^2$. It is the product of a Normal and a Inverse-Gamma distributions:

$p(\mu,\sigma^2) = N(\mu\|m,q)IG(\sigma^2\|a,b) \propto \frac{1}{q}^{\frac{1}{2}} \frac{1}{\sigma^2}^{a+1} \exp \left( -\frac{b}{\sigma^2} -\frac{1}{2 q}\left(\mu-m\right)^2 \right)$

Again, our data is Gaussian:

$p(x\|\mu,\sigma^2) = N(x\|\mu,\sigma^2) \propto \frac{1}{\sigma^2}^{\frac{1}{2}} \exp\left( -\frac{1}{2} \frac{(x-\mu)^2}{\sigma^2}\right)$

The posterior can be written as:

$p(\mu,\sigma^2\|x) \propto p(x\|\mu,\sigma^2)p(\mu,\sigma^2) \propto \frac{1}{\sigma^2}^{a+1+\frac{1}{2}} \frac{1}{q}^{\frac{1}{2}} \exp \left( -\frac{1}{2} \frac{(x-\mu)^2}{\sigma^2} \right) \exp \left(  -\frac{1}{2 q}\left(\mu-m\right)^2 \right) \exp \left( -\frac{b}{\sigma^2}\right)$

The problem is that the posterior is not conjugate anymore and so we cannot write an simple update expression. Given the way we wrote the prior, it would be usefull to approximate the posterior in the form $p(\mu,\sigma^2\|x) \approx Q(\mu)Q(\sigma^2)$: In other words, what are the functional forms of $Q(\mu)$ and $Q(\sigma^2)$ such that their product is as close as possible to the true posterior (which is not conjugate to out prior). This problem can be solved by variational inference. These distributions can be calculated as:

$Q(\mu) \propto \exp \left( \int \ln p(x,\mu,\sigma^2) Q(\sigma^2) \text{d}\sigma^2 \right)$

$Q(\sigma^2) \propto \exp \left( \int \ln p(x,\mu,\sigma^2) Q(\mu) \text{d}\mu \right)$

This just means that the distribution $Q(\mu)$ is proportional to the exponential of the expected value with respect to $\sigma^2$ (in some sense this is similar to a Gibbs sampler but we are _feeding_ the mean instead of samples).

Since $p(x,\mu,\sigma^2)=p(x\|\mu,\sigma^2)p(\mu,\sigma^2)$ we can replace the above expression in the equations. For the first term, since it defines a distribution on $\mu$, we can just keep the terms with $\mu$ to write:

$Q(\mu) \propto \exp \left( -\frac{1}{2}\left( \mu^2 \left( \mathbb{E}\left[\frac{1}{\sigma^2}\right]+\frac{1}{q}\right) -2\mu\left( x\mathbb{E}\left[\frac{1}{\sigma^2}\right]+\frac{m}{q} \right)\right) \right) \propto N(\mu\|m',q')$ 

If we do the same for $\sigma^2$

$Q(\sigma^2) \propto \frac{1}{\sigma^2}^{a+1+\frac{1}{2}}\exp \left( -\frac{1}{\sigma^2}\left( b+\frac{1}{2}(x^2-2x\mathbb{E}[\mu]+\mathbb{E}[\mu^2])\right) \right)$

and we can see that $Q(\sigma^2)=IG(\sigma^2\|a',b')$ with $a'=a+\frac{1}{2}$ and $b'=b+\frac{1}{2}\left( x^2-2xm'+q'+m'^2 \right)$.

Going back to the expression for $Q(\mu)$, we have a term with $\mathbb{E}\left[\frac{1}{\sigma^2}\right]$ which is equal to $\frac{a}{b}$ (because if $z$ is Inverse-Gamma then $1/z$ is Gamma) and so we can write that $m'=\frac{1}{q+s}\left(qx+ms\right)$ and $q'=\frac{qs}{q+s}$ (with $s=\frac{b'}{a'}$).

This defines a set of equations that can be used in a iterative way to determine the posterior parameters.



#### Dynamic model

Now, we can follow exactly the same logic as before to build the dynamical model to compute the filtering distribution $p(\mu_t,\sigma_t^2\|x_{1:t}) \propto p(x_t\|\mu_t,\sigma_t^2)p(\mu_t,\sigma_t^2\|x_{1:t-1})$.

Write the prior based on it's dependence on previous parameters values:

$p(\mu_t,\sigma_t^2\|x_{1:t-1})=\int p(\mu_t,\sigma_t^2\|\mu_{t-1},\sigma_{t-1}^2)p(\mu_{t-1},\sigma_{t-1}^2\|x_{1:t-1}) \text{d}\mu_{t-1}\text{d}\sigma_{t-1}^2$

In a similar fashion as we did in previous cases, suppose that, at $t-1$, we know the hidden variables distribution and let it have the following form:

$p(\mu_{t-1},\sigma_{t-1}^2\|x_{1:t-1}) = N(\mu\|m_{t-1},q_{t-1})IG(\sigma_{t-1}^2\|a_{t-1},b_{t-1})$

and that the transformation $p(\mu_t,\sigma_t^2\|\mu_{t-1},\sigma_{t-1}^2)$ is one such that:

$p(\mu_t,\sigma_t^2\|x_{1:t-1}) = N(\mu\|m_t^-,q_t^-)IG(\sigma_t^2\|a_t^-,b_t^-)$

with 

$m_t^-=m_{t-1}$

$q_t^- = \frac{q_{t-1}}{\phi}$

$a_t^-=\phi(a_{t-1}-1)+1$

$b_t^-=b_{t-1} = \phi b_{t-1}$

with $\phi \in [0,1]$. Following the same logic as we did previously, this transformation basically says that we are preserving expectations but increasing the variance/adding uncertainty: this makes sense as a way to propagate parameters forward. 

We want to approximate the posterior $p(\mu_t,\sigma_t^2\|x_{1:t})$ with $Q(\mu)Q(\sigma^2)$. Using the prior, likelihood and the previous formulas for variational inference, we can say that the posterior is

$p(\mu_t,\sigma_t^2\|x_{1:t})=Q(\mu)Q(\sigma^2)=N(\mu\|m_t,q_t)IG(\sigma_t\|a_t,b_t)$

with the posterior parameters respecting the following system of equations:


$s=\frac{b_t}{a_t}$

$a_t=a_t^- + \frac{1}{2}$

$b_t=b_t^- + \frac{1}{2}\left( x_t^2-2x_t m_t + q_t + m_t^2\right)$

$q_t=\frac{q_t^- s}{q_t^- + s}$

$m_t=\frac{1}{q_t^-+s}\left( q_t^- x_t + m_t^- s \right)$



#### Behaviour

We can notice that, after updating, $\mathbb{E}[\sigma_t^2]=\frac{b_t}{a_t-1}=\frac{b_t s}{b_t-s}$ which is a increasing function of variable $s=\frac{b_t}{a_t}$ (derivative is always positive). Converselly, $s=\frac{\mathbb{E}[\sigma_t^2] b}{\mathbb{E}[\sigma_t^2]+b}$ is also a increasing function of $\mathbb{E}[\sigma_t^2]$, i.e, if variance grows, $s$ increases as well.

With this, and looking of the formula for the mean update:

$m_t = \frac{q_t^-}{q_t^-+s} x_t + \frac{s}{q_t^-+s} m_t^-$

we can conclude that, if the variance increases, less weight is given to the most recent observation. This way we can say that the _lookback window_ is dynamic, depending on the current state and observation noise values. Another way to write this is making an analogy to the previous cases

$m_t = \lambda_t x_t + (1-\lambda_t) m_{t-1}$

where $\lambda_t$ gets smaller is volatility increases. 


```python
class NIG_VI_Track(object):
    def __init__(self,phi,m=0,var_init=1,window_init=20,n_iter=50,tol=1e-6):
        self.phi=phi
        self.a=1+1/(2*(1-self.phi)) # use the limit value
        self.m=m
        self.b=var_init*(self.a-1) 
        self.is_init=False
        self.n_iter=n_iter # VI iterations
        self.tol=tol
        self.window_init=window_init
        self.l=1
    
    def update(self,x):
        x=np.array(x)
        if isinstance(x,np.ndarray):            
            if not self.is_init:
                self.m=np.mean(x[-min(self.window_init,x.size-1):-1])
                v=np.var(x[-min(self.window_init,x.size-1):-1])
                # this seems to make sense
                # attribute a fraction of the variance to
                # the hidden state
                self.q=v*(1-self.phi)
                # the rest goes as observation noise
                self.b=self.phi*v*(self.a-1)                
                self.is_init=True
            x=x[-1]

        b=self.phi*self.b
        m=float(self.m)
        q=self.q/self.phi # make it larger..        
        self.l=1
        # control precision in s
        prev_s=self.b/self.a
        for i in range(self.n_iter):
            s=self.b/self.a
            self.b=b+0.5*(x*x-2*x*self.m+self.q+self.m*self.m)
            self.m=(q*x+m*s)/(q+s)
            self.l=s/(q+s)
            self.q=q*s/(q+s)
            # check for stopping
            s=self.b/self.a
            if np.abs(s/prev_s-1)<self.tol:
                break
            prev_s=s
        if i==self.n_iter-1:
            print('not converged!')

    @property
    def mean(self):
        return self.m
    
    @property
    def variance(self):
        return self.b/(self.a-1)

    @property
    def std(self):
        return np.sqrt(self.variance)
     
phi=0.8
window=10
nig_vi=NIG_VI_Track(phi,n_iter=20)    
roll=RollingEstimator(window)

mean_nig_vi=np.zeros(n)
mean_roll=np.zeros(n)

std_nig_vi=np.zeros(n)
std_roll=np.zeros(n)

l=np.zeros(n)

for i in range(window,n):
    nig_vi.update(x[:i+1])
    roll.update(x[:i+1])
    mean_nig[i]=nig_vi.mean
    mean_roll[i]=roll.mean
    std_nig[i]=nig_vi.std
    std_roll[i]=roll.std
    l[i]=nig_vi.l


fig, (ax1,ax2,ax3) = plt.subplots(3, sharex=True)
fig.set_size_inches(18.5, 15.5)

ax1.set_title('Mean')
ax1.plot(x[window:],label='Data')
ax1.plot(mean_nig[window:],label='NIG-VI Estimator')
ax1.plot(mean_roll[window:],label='Rolling Estimator')
ax1.legend()
ax1.grid(True)

ax2.set_title('Standard deviation')
ax2.plot(std_nig[window:],label='NIG-VI Estimator')
ax2.plot(std_roll[window:],label='Rolling Estimator')
ax2.legend()
ax2.grid(True)

ax3.set_title('Equivalent L')
ax3.plot(1-l[window:],label='Equivalent weight to recent observation')
ax3.legend()
ax3.grid(True)
plt.show()   

```


    
![png](/images/movavg/output_13_0.png)
    


