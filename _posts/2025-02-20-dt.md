# Decisions and Models

The point of developing models is to use them for a task. First we need to define well which task are we solving then try to model in such a way it can produce actions. Outcomes and observations have uncertainty associated with them: random errors in measurements, limited data, limited explanatory variables and limited modelling capacity are all factors that stir the need to use a probabilistic framework. 

Let's explore the task of making bets.


## Problem setup

An appropriate framework is one where, under finite resources, we are presented with a sequence of return opportunities $y$ and commit a fraction of the capital to it. After $n$ periods our capital is

$S_n = S_0(1+wy_1)(1+wy_2)\cdots \rightarrow S_0 \exp(nG)$

with $G = \mathbb{E}[\log (1+wy)]$. This expresses reinvestment and compounding and, a logical objective is to maximize the growth rate $G$. With this log utility we make a _infinite_ penalty for $w$ that can produce zero capital: at some point if our capital goes to zero we cannot bet again (if this condition is assured then time averages should be equivalent to ensemble averages). 

Using the log-utility gives us a defined answer for the amount we should bet and a resonable (and non trivial) approximation is to expand up to second order (note now that we are considering a vector of possible bets and a corresponding vector $w$ of amounts to commit in each one):

$G = \mathbb{E}\[\log (1+w^Ty)] \approx \mathbb{E}\[w^Ty - \frac{1}{2}(w^Ty)^2]$

Since we want to maximize $G$ we can compute the optimal decision $w$:

$w^* = \left\[ \int yy^T p(y) \text{d}y \right\]^{-1} \left\[ \int y p(y) \text{d}y \right\]$

If $y$ depends on some other (observable) variables $x$, we can write

$w^\* = \left\[ \int yy^T p(y\|x) \text{d}y \right\]^{-1} \left\[ \int y p(y\|x) \text{d}y \right\] = M\_{y\|x}^{-1}\mu\_{y\|x}$

where $M\_{y\|x}$ is the conditional second non central moment and $\mu\_{y\|x}$ the conditional expected value.


## Decision Theory

In the previous discussion we assumed the distribution of $y$ (or $y\|x$) is known but in practice it has to be determined from data. The typical setup is one where we have a training set $D$ which consist of observations of returns and the associated features at different time instants. The objective is to, given $D$, find a way to maximize growth rate when presented with new bets:

$G = \mathbb{E}\[w_D^Ty - \frac{1}{2}(w_D^Ty)^2\]$

where the notation $w_D$ means that the optimal decisions were estimated with the training dataset $D$ and the expectation is taken with respect to the true but unknown distribution.

There are two different ways to tackle the problem: _learn_ the weight directly from data or _model_ the distribution and use the implied optimal weight.

### Model the weight

It can make more sense (and simple to explain) to just assume some functional form for the weight and optimize the parameters. This is the typical _empirical risk minimization_ in machine learning (which should be solved with regularization but that's not the point here). In general, let's say that $w$ depends on some parameters $\phi$, that need to be estimated. Since we don't know the distribution and are not looking to estimate hidden parameters, we proceed directly with the approximation to expected value:

$\phi^* = \text{argmax}\_{\phi} \frac{1}{n} \sum_{i \in D} \left( w_{\phi}(x_i)^T y_i - \frac{1}{2} (w_{\phi}(x_i)^T y_i)^2 \right)$

This setup is used many times: we define a model (or rule if we want) to bet on a trade based on some information and we try to _optimize_ to generate more money.


#### Example

Let $y,x$ be scalars and assume a linear model for the weight $w = ax$ (this idea just states that weight is proportional to the magnitude of the feature; of course, there are simpler examples, like deciding just to go long or short with the available money). Given a set of examples, maximizing $G$ yields the estimator:

$\hat{a} = \frac{\sum x_i y_i}{ \sum x_i^2 y_i^2}$

This is similar to a linear regression estimator with some penalization in the denominator related to how $y$ flucuates: this term is controling for excessive leverage. 

Lets generate some data with a true distribution $y\|x \sim N(cx, \sigma^2)$ to estimate the betting parameter; also, we will generate some data to test out of sample as well.



```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(6)
# simulate data
# y = c*x + noise || p(y|x) ~ N(cx, sigma^2)
c = 0.1
x_noise_scale = 0.1
y_noise_scale = 0.01 # 1 percent
n = 1000
# generate train data
x_train = np.random.normal(0, x_noise_scale, n)
y_train = c*x_train + np.random.normal(0, y_noise_scale, n)
# generate test data
x_test = np.random.normal(0, x_noise_scale, n)
y_test = c*x_test + np.random.normal(0, y_noise_scale, n)
# estimate weight model
a = np.sum(x_train*y_train)/np.sum(np.power(x_train,2)*np.power(y_train,2))
print('a estimate: ', np.round(a,2))
w_emp_opt = a*x_test
# generate strategy on test data
s_emp = y_test*w_emp_opt
# calculate growth rate out of sample
g_emp_oos = np.mean(s_emp) - 0.5*np.mean(np.power(s_emp,2))
print('OOS G: ', np.round(g_emp_oos,3))
```

    a estimate:  275.32
    OOS G:  0.125
    

### Model the distribution

The other approach we can take is to fit a parametric distribution, $p(y,x\|\theta)$ (if we model directly the conditional - a regression - we can write $p(y\|x, \theta)$ ). This can be done with the usual machinery $p(\theta\|D) \propto p(D\|\theta)p(D)$. We can use point estimates of parameters $\theta^* = \text{argmax}_{\theta} p(\theta\|D)$ (MLE or MAP depending on assumptions) or use the full distribution with parameter uncertainty to compute the moments needed for optimal weight (full posterior predictive distributions). 

In this approach, __modelling is separate from action__ as parameters are selected based on how they adjust to the dataset in a probabilistic sense; the subsequent action is derived on the fly as new data comes.

#### Example

Let's pick the previous case and model the conditional as $y\|x \sim N(cx, \sigma^2)$. The maximum likelihood estimators are:

- $\hat{c} = \frac{\sum_i x_i y_i}{\sum_i x_i^2}$
- $\hat{\sigma^2} = \text{Var}\[y-\hat{c}x\]$

When presented with a new $x$, the optimal weight is

$w = \frac{\hat{c}x}{\sigma^2 + (\hat{c}x)^2}$

We can make a interesting observation here: both in the empirical and distributional approaches we assumed a linear model - both _models_ have the same complexity - but when we modelled the distribution, our optimal decision became non linear. To match the same response we would have to consider a more complex function for $w(x)$ in the empirical case. 

In this case, the data was generated by this process, so it is normal that this procedure yields better results. The point here is to compare different approaches for the problem.



```python
# go back to the previous example
# p(y\|x) ~ N(cx, sigma^2)
# of course here modelling the distribution will give better
# results than the empirical procedure because we know that 
# the data was generated from it. The point is that they are different
# and using the distribution get a non linear response!

# estimate linear model
c_est = np.sum(x_train*y_train)/np.sum(x_train*x_train)
sigma_est = np.std(y_train - c_est*x_train)

# evaluate on test data
# optimal weights for each point
w_d_opt = c_est*x_test/(sigma_est*sigma_est + np.power(c_est*x_test,2))
s_d = y_test*w_d_opt

# calculate growth rate out of sample
g_d_oos = np.mean(s_d) - 0.5*np.mean(np.power(s_d,2))
print('OOS G: ', np.round(g_d_oos,3))

plt.plot(np.cumsum(s_d), label = 'Strategy w/ Distribution')
plt.plot(np.cumsum(s_emp), label = 'Strategy w/ Empirical')
plt.grid(True)
plt.legend()
plt.show()


plt.plot(w_emp_opt, w_d_opt,'.')
plt.grid(True)
plt.xlabel('empirical w')
plt.ylabel('distributional w')
plt.show()

```

    OOS G:  0.183
    


    
![png](/images/dt/output_5_1.png)



    
![png](/images/dt/output_5_2.png)
    

Again, we can see that, if we get a very large input, the weight decreases towards zero: this makes sense from practical prespective as these observations tend to be quite outside of the training data, hence, it is more difficult to be sure on what to do (although the reason the model does this is to prevent too much leverage as fluctuations would become too large; a full bayesian approach would incorporate this uncertainty naturally but the effect is similar).

#### Model with posterior

The problem of model inference can be made with the full posterior $p(\theta\|D) \propto p(D\|\theta)p(D)$ instead of a point estimate of parameters as we did previously. For more complicated models we can design a sampler for this distribution (as we saw in other posts) but there is another advantage of using a sampler which we will see next. For now, let us consider the previous (simple) problem, and sample from the posterior to estimate the parameters distribution that is consistent with the data; we can use a normal prior for $c$ and and a inverse-gamma for $\sigma^2$ as prior (they are conjugate). Let's implement this.



```python

# Prior parameters
mu_0 = 0  # Prior mean for c
tau_0_sq = 1  # Prior variance for c
alpha_0 = 2  # Prior shape for sigma^2
beta_0 = 0.01  # Prior scale for sigma^2

# Gibbs Sampler settings
num_samples = 5000
burn_in = 1000

# Initialize parameters
c_samples = np.zeros(num_samples)
sigma_sq_samples = np.zeros(num_samples)
c_current = 0.0
sigma_sq_current = 1.0

for i in range(num_samples):
    # Sample c given sigma^2
    tau_n_sq = 1 / (1/tau_0_sq + np.sum(x_train**2) / sigma_sq_current)
    mu_n = tau_n_sq * (mu_0/tau_0_sq + np.sum(x_train * y_train) / sigma_sq_current)
    c_current = np.random.normal(mu_n, np.sqrt(tau_n_sq))
    
    # Sample sigma^2 given c
    alpha_n = alpha_0 + n / 2
    beta_n = beta_0 + 0.5 * np.sum((y_train - c_current * x_train) ** 2)
    sigma_sq_current = 1 / np.random.gamma(alpha_n, 1/beta_n)
    
    # Store samples
    c_samples[i] = c_current
    sigma_sq_samples[i] = sigma_sq_current

# Discard burn-in samples
c_samples = c_samples[burn_in:]
sigma_sq_samples = sigma_sq_samples[burn_in:]

# Plot posterior distributions
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.hist(c_samples, bins=30, density=True, alpha=0.5)
plt.axvline(c, color='r', linestyle='--', label=f'True c={c}')
plt.axvline(np.mean(c_samples), color='g', linestyle='--', label=f'Estimate c={np.mean(c_samples):.4f}')
plt.xlabel("c")
plt.ylabel("Density")
plt.title("Posterior Distribution of c")
plt.legend()

plt.subplot(1, 2, 2)
plt.hist(1000*sigma_sq_samples, bins=30, density=True, alpha=0.5)
plt.axvline(1000*y_noise_scale**2, color='r', linestyle='--', label=f'True sigma^2={1000*y_noise_scale**2:.4f}')
plt.axvline(1000*np.mean(sigma_sq_samples), color='g', linestyle='--', label=f'Estimate sigma^2={1000*np.mean(sigma_sq_samples):.4f}')
plt.xlabel("sigma^2 X 1000")
plt.ylabel("Density")
plt.title("Posterior Distribution of sigma^2")
plt.legend()

plt.show()

```


    
![png](/images/dt/output_7_0.png)
    


This model is quite simple and we did not get nothing new that we could not already get using closed form expressions: we can compute the full posterior analitycally and use that in the decision process in place of the point estimate.

#### Penalizations with actual utility

Since models are an approximation of reality, it may seem odd, simplistic or too dogmatic to fit in a probabilistic sense only and forgot the fact that the model is going to be used for something. Maybe there is one feature that generates all the money and maybe the model trades too much (we can penalize the utility with transaction costs!). One approach to solve this is to do a importance resampling of $p(\theta\|D)$ taking into account the utility. After all, the samples represent possible values for the parameters; giving more weight to the probabilistic plausible parameters that also produce good utilities makes sense. So, if we use a sampler to get information on the posterior, then we can do:

- for each sample, compute the weights it produce
- evaluate $G$ for each one
- Compute an importance for each sample with $\propto \exp(G)$ and normalize
- resample with importance, effectivelly making the best parameters in the $G$ sense appear more times

Let's do this for the previous example.


```python
# Importance Resampling Based on Utility Function G
w_samples = (c_samples[:, None] * x_train) 
w_samples /= (sigma_sq_samples[:, None] + (c_samples[:, None] * x_train)**2)
G_samples = np.mean(w_samples * y_train - 0.5 * (w_samples * y_train) ** 2, axis=1)

# Compute importance weights
importance_weights = np.exp(G_samples - np.max(G_samples))  # Normalize for numerical stability
importance_weights /= np.sum(importance_weights)

# Resample indices based on importance weights
resampled_indices = np.random.choice(len(c_samples), size=len(c_samples), p=importance_weights)
c_resampled = c_samples[resampled_indices]
sigma_sq_resampled = sigma_sq_samples[resampled_indices]

# Plot posterior distributions after resampling
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.hist(c_resampled, bins=30, density=True, alpha=0.7)
plt.axvline(c, color='r', linestyle='--', label=f'True c={c}')
plt.axvline(np.mean(c_resampled), color='g', linestyle='--', label=f'Re-Estimate c={np.mean(c_resampled):.4f}')
plt.xlabel("c")
plt.ylabel("Density")
plt.title("Resampled Posterior Distribution of c")
plt.legend()

plt.subplot(1, 2, 2)
plt.hist(1000*sigma_sq_resampled, bins=30, density=True, alpha=0.7)
plt.axvline(1000*y_noise_scale**2, color='r', linestyle='--', label=f'True sigma^2={1000*y_noise_scale**2:.4f}')
plt.axvline(1000*np.mean(sigma_sq_resampled), color='g', linestyle='--', label=f'Re-Estimate sigma^2={1000*np.mean(sigma_sq_resampled):.4f}')

plt.xlabel("sigma^2")
plt.ylabel("Density")
plt.title("Resampled Posterior Distribution of sigma^2")
plt.legend()

plt.show()

```


    
![png](/images/dt/output_9_0.png)
    


Of course, in this simple problem, we did not see anything different but in more complex models it can help producing better models. This is interesting as we are blending together direct utility optimization with probabilistic modelling; also, we can include some penalizations in the samples (transactions costs, drawdowns, sparse solutions) and the good part is that we do not need to develop a new sampling algorithm (let's say that in the end we are doing an educated brute force search).

## Which route to take?

There is no way to decide which strategy is better. Should we model the decisions directly or model the distribution and use it to generate decisions? For sure model the decisions is more intuitive but if we happen to have the true model (we will not) then we can perform optimally. If we opt to model distributions we can gain more intuition on the problem and transfer that knowledge to another investment scenarios and also it is clearer what we are trying to do (instead of fitting some complicated function; in a world with signals so small as financial returns maybe it can help to think on what are we modelling). The distribution prespective can be criticized because we need to assume a model for it but we also need to make a decision on the functional form for $w$ anyway and so I don't believe we can settle it from there.

What I think is a good observation to lean more into model distributions is the case were we model the conditional with some features $p(y\|x) = N(f(x), \sigma^2)$. An equivalent complexity model for $w$ will be one where $y = g(x)$; it makes sense to expect $g(x)$ to be somewhat similar to  $\sim kf(x)$. Under this, the optimal decision from modelling the distribution is more complex and nuanced with the same number of parameters $w(x) = \frac{f(x)}{\sigma^2 + f(x)^2}$. Also, it is closer to a desirable solution by common logic where we naturally reduce our exposure to zero with inputs far away from the observations. 

A simpler procedure that encodes a more complete decision seems a better choice.


