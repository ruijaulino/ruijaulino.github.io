# A model for economic data

Economic indicators are believed to yield a reading of current conditions and many people in different positions look at them to take decisions. In general, this data is difficult to model and makes it's interpretation subjective. For someone interested in building models and possibly trading them this is of no great use. 

From a trading prespective the most obvious choice to make is to study the behaviour of some risk asset near the release (the only information that matters is that it happened and/or the magnitude) and try to find regularities (for example, the asset _mean reverts_ after the release); the objective in this post is for a more longer term view where one would be interested in compressing the information into something interpretable.

The first obstacle is the evident lack of a definite release frequency among indicators. A way to deal with this jungle is to resample to make observations coincide and take conclusions on how they vary together - this loses information and uses the data in a poor way; also, it does not mimic quite well the true process: in reality, we will probably want to revise our positions/expectations when a new indicator comes (even if the others have not updated yet). The objective here is to make a model that can incorporate this idiosyncrasies of the datasets and allow it's estimation.

A model that could make sense for this type of data is to assume observations are clustered in time. In this case, we are assuming that, for example, a reading of inflation of magnitude $x$ tends to be _near_ (in time) readings of GDP of about $y$; even if one of the series is monthly and the other quarterly, if we assume that the dynamics are slowly changing (at least slow _enough_ for the model to make sense) then we could model them as a hidden markov model but for every _instant_ we do not have observations in all variables. For example, if the emission model is a Gaussian, we may not be able to calculate a covariance between observations because they do not happen at the same time but for sure we can compute if is possible to _group_ them in time according to individual moments like the mean or scale.

For example, consider the following code.


```python
# imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from numba import jit
from scipy.stats import invwishart

def simulate_hmm(n,A,P,means,covs):
    '''
    n: integer with the number of points to generate
    A: numpy (n_states,n_states) array with transition prob
    P: numpy (n_states,) array with init state prob
    means: list with len=n_states of numpy (n_states,) array with the means for each
        variable
    covs:list with len=n_states of numpy (n_states,n_states) array with the covariances
        for each variable
    '''	
    states=np.arange(A.shape[0],dtype=int)
    z=np.zeros(n,dtype=int)
    x=np.zeros((n,means[0].size))
    z[0]=np.random.choice(states,p=P)
    x[0]=np.random.multivariate_normal(means[z[0]],covs[z[0]])
    for i in range(1,n):
        z[i]=np.random.choice(states,p=A[z[i-1]])
        x[i]=np.random.multivariate_normal(means[z[i]],covs[z[i]])
    return x,z

# Generate the observations
A=np.array([[0.9,0.1],[0.1,0.9]]) # state transition
P=np.array([0.5,0.5]) # initial state distribution
means=[np.array([-0.5,0.5]),np.array([0.5,-0.5])]
# list of covariance matrices (for each mixture)
# let the data have a similar covariance structure
# in both states
covs=[
    np.array([[0.1,0.05],[0.05,0.1]]),
    np.array([[0.1,0.05],[0.05,0.1]]),
]
# number of data points to generate
n=1000 
x,z=simulate_hmm(n,A,P,means,covs)
# damage observations
xd=np.array(x)
for i in range(xd.shape[0]):
    j=np.random.choice([0,1])
    xd[i,j]=np.nan
print(xd)
```

    [[-0.78282149         nan]
     [        nan  0.35807569]
     [        nan  0.74212361]
     ...
     [-0.97698908         nan]
     [-0.54783579         nan]
     [        nan  0.56714118]]
    

What we did was to generate observations from a HMM but then we _damaged_ the observations in order to have data for one of the variables only at each time instant. Can we still learn the time structure of the model? Can we recover the transition matrix and observe that the variables have positive and negative means in both states? 

Of course we cannot make conclusions regarding the covariance because we never have coincident observations but that does not prevent us to learn the rest of the structure. Now we describe how one could modify the Gibbs sampler for the HMM to acomodate for this (the discussion here follows the previous post and much of the code is similar).

How can we achieve this? To estimate the HMM, we need the probability that the sequence was generated from a given state; since some observations may not contain realizations of all variables we can just consider that, within a state, the observations are independent and so we just need to multiply the probabilities - if the variable does not contain a realization just keep it equal to one. To sample from the emissions we just need to consider the variables individually and only the indexes where samples exist. This is illustrated in the following code, which is an adaptation of the code made on the previous post.


```python
PROB_NUM_PREC=1e-8
def mvgauss_prob(x,mean,cov_inv,cov_det):
    '''
    x: numpy (n,p) array 
        each row is a joint observation
    mean: numpy (p,) array with the location parameter
    cov_inv: numpy(p,p) array with the inverse covariance 
            can be computed with np.linalg.inv(cov)
    cov_det: scalar with the determinant of the covariance matrix 
            can be computed with np.linalg.det(cov)
    returns:
        out: numpy(n,) array where each value out[i] is the 
                probability of observation x[i] 
    '''
    k=mean.size # number of variables
    x_c=x-mean # center x
    # vectorized computation     
    out=np.exp(-0.5*np.sum(x_c*np.dot(x_c,cov_inv),axis=1))/np.sqrt(np.power(2*np.pi,k)*cov_det)
    out[out<PROB_NUM_PREC]=PROB_NUM_PREC
    return out

# this can be compiled
@jit(nopython=True)
def forward(prob,A,P):
    '''
    Forward algorithm for the HMM
    prob: numpy (n,n_states) array with
        the probability of each observation
        for each state
    A: numpy (n_states,n_states) array with the state
        transition matrix
    P: numpy (n_states,) array with the initial
        state probability
    returns:
        alpha: numpy (n,n_states) array meaning
            p(state=i|obs <= i)
        c: numpy (n,) array with the normalization
            constants
    '''
    n_obs=prob.shape[0]
    n_states=prob.shape[1]
    alpha=np.zeros((n_obs,n_states),dtype=np.float64)
    c=np.zeros(n_obs,dtype=np.float64)
    alpha[0]=P*prob[0]
    c[0]=1/np.sum(alpha[0])
    alpha[0]*=c[0]
    for i in range(1,n_obs):
        alpha[i]=np.dot(A.T,alpha[i-1])*prob[i] 
        c[i]=1/np.sum(alpha[i])
        alpha[i]*=c[i]
    return alpha,c

# this can be compiled
@jit(nopython=True)
def backward_sample(A,alpha,q,transition_counter,init_state_counter):
    '''
    Backward sample from the state transition matrix and state sequence
    A: numpy (n_states,n_states) array with the state
        transition matrix
    alpha: numpy (n,n_states) array meaning
        p(state=i|obs <= i)		
    q: numpy (n,) to store the sample of state sequence
    transition_counter: numpy (n_states,n_states) array to store 
        transition counts to be used to sample a state transition 
        matrix
    init_state_counter: numpy (n_states,) array to store the
        number of times state i is the initial one
    returns:
        none (q and transition_counter are changed inside this function)
    '''	
    # backward walk to sample from the state sequence
    n=q.size
    # sample the last hidden state with probability alpha[-1]
    q[n-1]=np.searchsorted(np.cumsum(alpha[-1]),np.random.random(),side="right")#]
    # aux variable
    p=np.zeros(A.shape[0],dtype=np.float64)
    # iterate backwards
    for j in range(n-2,-1,-1):
        # from formula
        p=A[:,q[j+1]]*alpha[j] 
        # normalize (from formula)
        p/=np.sum(p) 
        # sample hidden state with probability p
        q[j]=np.searchsorted(np.cumsum(p),np.random.random(),side="right")
        # increment transition counter (we can do this calculation incrementally)
        transition_counter[q[j],q[j+1]]+=1 
    # increment initial state counter
    init_state_counter[q[0]]+=1
```


```python
class GaussianHMM(object):
    def __init__(self,
                 n_states=2,
                 n_gibbs=1000,
                 A_zeros=[],
                 A_groups=[],
                 f_burn=0.1,
                 max_k=0.25,
                 pred_l=None,
                 irregular_obs=False,
                 independent_vars=False,
                 **kwargs):
        '''
        n_states: integer with the number of states
        n_gibbs: integer with the number of gibbs iterations
        A_zeros: list of list like [[0,0],[0,1],[3,1]]
            with the entries of the transition matrix that are 
            set to zero
        A_groups: list of lists like [[0,1],[2,3]] of disjoint elements
            where each sublist is the set of states that have the same 
            emissions, i.e, they are the same state
        f_burn: float in (0,1) with the fraction of points to burn at
            the beginning of the samples
        max_k: covariance shrinkage parameter
        pred_l: None or integer with the number of past observations
            used to make a prediction
        irregular_obs: boolean with the indication to consider
            that the observations may contain missing values
            This needs to be handled differently 
            independent_vars is set to True
        independent_vars: boolean with the indication to consider that
            observations are independent inside each state
            This should simplify the inference as now the likelihood is 
            just the product of prob
            Should be true if irregular_obs=True    
        '''
        self.n_states=n_states
        self.f_burn=f_burn
        self.n_gibbs=n_gibbs
        self.A_zeros=A_zeros
        self.A_groups=A_groups
        self.pred_l=pred_l
        if len(self.A_groups)==0:
            self.A_groups=[[e] for e in range(self.n_states)]   
        self.eff_n_states=len(self.A_groups)
        self.max_k=max_k
        self.independent_vars=independent_vars
        self.irregular_obs=irregular_obs        
        if self.irregular_obs:
            self.independent_vars=True
        self.n_gibbs_sim=int(self.n_gibbs*(1+self.f_burn))
        self.p=1
        self.P=None
        self.gibbs_P=None
        self.gibbs_A=None
        self.gibbs_mean=None
        self.gibbs_cov=None
        self.A=None
        self.states_mean=None
        self.states_cov=None 
        self.states_cov_inv=None
        self.states_cov_det=None
        self.w_norm=1

    def view(self,plot_hist=False,covs=True):
        '''
        plot_hist: if true, plot histograms, otherwise just show the parameters
        '''
        print('** GIBBS SAMPLER (HMM) **')
        print('GROUPS')
        for e in self.A_groups:
            print('- States %s have the same emission'%','.join([str(a) for a in e]))
        print('INITIAL STATE PROBABILITY')
        print(self.P)
        if plot_hist:
            for i in range(self.n_states):
                plt.hist(self.gibbs_P[:,i],density=True,alpha=0.5,label='P[%s]'%(i))
            plt.legend()
            plt.grid(True)
            plt.show()
        print('STATE TRANSITION MATRIX')
        print(self.A)
        print()
        if plot_hist:
            for i in range(self.n_states):
                for j in range(self.n_states):
                    if [i,j] not in self.A_zeros:
                        plt.hist(self.gibbs_A[:,i,j],density=True,alpha=0.5,label='A[%s->%s]'%(i,j))
            plt.legend()
            plt.grid(True)
            plt.show()
        for j in range(self.eff_n_states):
            print('STATE %s'%(j+1))
            print('MEAN')
            print(self.states_mean[j])
            if covs:
                print('COVARIANCE')
                print(self.states_cov[j])
                print()
            if plot_hist:
                if self.gibbs_mean is not None:
                    for i in range(self.p):
                        plt.hist(self.gibbs_mean[j,:,i],density=True,alpha=0.5,label='Mean x%s'%(i+1))
                    plt.legend()
                    plt.grid(True)
                    plt.show()
                if covs:
                    if self.gibbs_cov is not None:
                        for i in range(self.p):
                            for q in range(i,self.p):
                                plt.hist(self.gibbs_cov[j,:,i,q],density=True,alpha=0.5,label='Cov(x%s,x%s)'%(i+1,q+1))
                        plt.legend()
                        plt.grid(True)
                        plt.show()

    def next_state_prob(self,y,l=None):
        '''
        computes a vector with the next state probability
        given a input sequence
        y: numpy (n,self.p) array with observations
        l: integer to filter recent data in y -> y=y[-l:]
        '''
        assert y.ndim==2,"y must be a matrix"
        # just return the initial state probability 
        if y.shape[0]==0:
            return self.P
        assert y.shape[1]==self.p,"y must have the same number of variables as the training data"
        if l is not None:
            y=y[-l:]
        if self.states_cov_inv is None:
            self.states_cov_inv=np.zeros((self.eff_n_states,self.p,self.p))			
            self.states_cov_det=np.zeros(self.eff_n_states)
            for s in range(self.eff_n_states):
                self.states_cov_inv[s]=np.linalg.inv(self.states_cov[s])
                self.states_cov_det[s]=np.linalg.det(self.states_cov[s])
        n=y.shape[0]
        # declare arrays
        # probability of observations given state
        prob=np.zeros((n,self.n_states),dtype=np.float64) 
        # probability of observations given state		
        eff_prob=np.zeros((n,self.eff_n_states),dtype=np.float64) 
        for s in range(self.eff_n_states):
            # use vectorized function
            eff_prob[:,s]=mvgauss_prob(
                                        y,
                                        self.states_mean[s],
                                        self.states_cov_inv[s],
                                        self.states_cov_det[s]
                                        )
            prob[:,self.A_groups[s]]=eff_prob[:,[s]] 
        # compute non nan indexes
        non_nan_indexes=[]        
        if self.irregular_obs:
            for i in range(self.p):
                non_nan_indexes.append(~np.isnan(y[:,i]))        
        for s in range(self.eff_n_states):
            if self.independent_vars:
                # emission variables are independent
                # we can process this more easily by just multiplying probs
                # also, if there are nan's in the data we can solve the problem
                # here as well
                p_tmp=np.ones(prob.shape[0])
                for pi in range(self.p):
                    cov_inv=np.array([[1/self.states_cov[s][pi,pi]]])
                    cov_det=self.states_cov[s][pi,pi]
                    if self.irregular_obs:                               
                        p_tmp[non_nan_indexes[pi]]*=mvgauss_prob(
                                                        y[non_nan_indexes[pi],pi][:,None],
                                                        np.array([self.gibbs_mean[s,i-1][pi]]),
                                                        cov_inv,cov_det)                         
                    else:
                        p_tmp*=mvgauss_prob(
                                        y[:,pi][:,None],
                                        np.array([self.states_mean[s][pi]]),
                                        cov_inv,
                                        cov_det)                         
                eff_prob[:,s]=p_tmp
                prob[:,self.A_groups[s]]=eff_prob[:,[s]] 
            else:
                eff_prob[:,s]=mvgauss_prob(
                                            y,
                                            self.states_mean[s],
                                            self.states_cov_inv[s],
                                            self.states_cov_det[s]
                                            )
                prob[:,self.A_groups[s]]=eff_prob[:,[s]]                       
        alpha,_=forward(prob,self.A,self.P)
        next_state_prob=np.dot(self.A.T,alpha[-1])  
        return next_state_prob
    
    def predict(self,y,**kwargs):
        '''
        Make a prediction for the next 
        observation distribution
        '''
        next_state_prob=self.next_state_prob(y,self.pred_l)
        # group next state prob
        tmp=np.zeros(self.eff_n_states)
        for i,e in enumerate(self.A_groups):
            tmp[i]=np.sum(next_state_prob[e])
        next_state_prob=tmp
        # compute expected value
        mu=np.sum(self.states_mean*next_state_prob[:,None],axis=0)
        # compute second central moment of the mixture distribution
        cov=np.zeros((self.p,self.p))
        for s in range(self.eff_n_states):
            cov+=(next_state_prob[s]*self.states_cov[s])
            cov+=(next_state_prob[s]*self.states_mean[s]*self.states_mean[s][:,None])
        cov-=(mu*mu[:,None])      
        return mu,cov
    
    def estimate(self,y,idx=None,**kwargs):	 
        '''
        Estimate the HMM parameters with Gibbs sampling
        y: numpy (n,p) array
            each row is a joint observation of the variables
        idx: None or array with the indexes that define subsequences
            for example, idx=[[0,5],[5,12],[12,30]] means that subsequence 1 is y[0:5],
            subsequence 2 is y[5:12], subsequence 3 is y[12:30], ...				   
        '''
        assert y.ndim==2,"y must be a matrix"
        if idx is None:
            idx=np.array([[0,y.shape[0]]],dtype=int)
        # just form safety
        idx=np.array(idx,dtype=int)
        n_seqs=idx.shape[0]
        self.states_cov_inv=None
        self.states_cov_det=None
        n=y.shape[0]
        self.p=y.shape[1]
        # initial state probabilities will not be sampled
        # a equal probability will be assumed
        self.P=np.ones(self.n_states)
        self.P/=np.sum(self.P)
        # generate variable with the possible states
        states=np.arange(self.n_states,dtype=np.int32)
        # create list to store the indexes where each variable is
        # not nan. This is used in next steps as well
        non_nan_indexes=[]
        # compute data covariance
        if self.irregular_obs:
            # if observations are irregular then we must filter for non NaN values
            c=np.zeros((self.p,self.p))
            for i in range(self.p):
                nniv=~np.isnan(y[:,i])
                non_nan_indexes.append(nniv)
                c[i,i]=np.var(y[nniv,i])
        else:
            c=np.cov(y.T,ddof=0)
        # fix when y has only one column
        if c.ndim==0:
            c=np.array([[c]])

        c_diag=np.diag(np.diag(c)) # diagonal matrix with the covariances
        # Prior distribution parameters
        # these parameters make sense for the type of problems
        # we are trying to solve - assuming zero correlation makes sense
        # as a prior and zero means as well due to the low 
        # values of financial returns
        m0=np.zeros(self.p) # mean: prior location (just put it at zero...)
        V0=c_diag.copy() # mean: prior covariance
        S0aux=c_diag.copy() # covariance prior scale (to be multiplied later)
        alpha0=self.n_states
        alpha=1 # multinomial prior (dirichelet alpha)
        zero_alpha=0.001 # multinomial prior (dirichelet alpha) when there is no transition
        alpha_p=0.05 # multinomial prior (dirichelet alpha) for init state distribution
        # Precalculations
        # the prior alphas need to be calculated before
        # because there may be zero entries in the A matrix
        alphas=[]
        for s in range(self.n_states):
            tmp=alpha*np.ones(self.n_states)
            for e in self.A_zeros:
                if e[0]==s:
                    tmp[e[1]]=zero_alpha
            alphas.append(tmp)
        invV0=np.linalg.inv(V0)
        invV0m0=np.dot(invV0,m0)
        self.eff_n_states=len(self.A_groups)
        # initialize containers
        transition_counter=np.zeros((self.n_states,self.n_states)) # counter for state transitions
        init_state_counter=np.zeros(self.n_states) # counter for initial state observations
        eff_prob=np.zeros((n,self.eff_n_states)) # probability of observations given state
        prob=np.zeros((n,self.n_states),dtype=np.float64) # probability of observations given state
        forward_alpha=np.zeros((n,self.n_states),dtype=np.float64)
        forward_c=np.zeros(n,dtype=np.float64)		
        self.gibbs_cov=np.zeros((self.eff_n_states,self.n_gibbs_sim,self.p,self.p)) # store sampled covariances
        self.gibbs_mean=np.zeros((self.eff_n_states,self.n_gibbs_sim,self.p)) # store sampled means
        self.gibbs_A=np.zeros((self.n_gibbs_sim,self.n_states,self.n_states)) # store sampled transition matricess
        self.gibbs_P=np.zeros((self.n_gibbs_sim,self.n_states))
        # initialize covariances and means
        for s in range(self.eff_n_states):
            self.gibbs_mean[s,0]=m0
            self.gibbs_cov[s,0]=c   
        # initialize state transition
        # assume some persistency of state as a initial parameter
        # this makes sense because if this is not the case then this is
        # not very usefull
        if len(self.A_zeros)==0:
            init_mass=0.9
            tmp=init_mass*np.eye(self.n_states)
            remaining_mass=(1-init_mass)/(self.n_states-1)
            tmp[tmp==0]=remaining_mass
            self.gibbs_A[0]=tmp
        else:
            # initialize in a different way!
            tmp=np.ones((self.n_states,self.n_states))
            for e in self.A_zeros:
                tmp[e[0],e[1]]=0
            tmp/=np.sum(tmp,axis=1)[:,None]
            self.gibbs_A[0]=tmp
        self.gibbs_P[0]=np.ones(self.n_states)
        self.gibbs_P[0]/=np.sum(self.gibbs_P[0])
        # create and initialize variable with
        # the states associated with each variable
        # assume equal probability in states
        q=np.random.choice(states,size=n)
        # previous used parameters to sample when there are not observations
        # on that state
        prev_mn=np.zeros((self.n_states,self.p))
        prev_Vn=np.zeros((self.n_states,self.p,self.p))
        prev_vn=np.zeros(self.n_states)
        prev_Sn=np.zeros((self.n_states,self.p,self.p))
        for j in range(self.n_states):
            prev_mn[j]=m0
            prev_Vn[j]=V0
            prev_vn[j]=self.p+1+1
            prev_Sn[j]=S0aux
        # Gibbs sampler
        for i in range(1,self.n_gibbs_sim):
            transition_counter*=0 # set this to zero
            init_state_counter*=0 # set this to zero
            # evaluate the probability of each
            # observation in y under the previously 
            # sampled parameters
            for s in range(self.eff_n_states):
                if self.independent_vars:
                    # emission variables are independent
                    # we can process this more easily by just multiplying probs
                    # also, if there are nan's in the data we can solve the problem
                    # here as well
                    p_tmp=np.ones(prob.shape[0])
                    for pi in range(self.p):
                        cov_inv=np.array([[1/self.gibbs_cov[s,i-1][pi,pi]]])
                        cov_det=self.gibbs_cov[s,i-1][pi,pi]
                        if self.irregular_obs:                               
                            p_tmp[non_nan_indexes[pi]]*=mvgauss_prob(
                                                            y[non_nan_indexes[pi],pi][:,None],
                                                            np.array([self.gibbs_mean[s,i-1][pi]]),
                                                            cov_inv,
                                                            cov_det) 
                        else:
                            p_tmp*=mvgauss_prob(
                                            y[:,pi][:,None],
                                            np.array([self.gibbs_mean[s,i-1][pi]]),
                                            cov_inv,
                                            cov_det)                         
                    eff_prob[:,s]=p_tmp
                    prob[:,self.A_groups[s]]=eff_prob[:,[s]]
                else:
                    # compute inverse and determinant
                    cov_inv=np.linalg.inv(self.gibbs_cov[s,i-1])
                    cov_det=np.linalg.det(self.gibbs_cov[s,i-1])
                    # use vectorized function
                    eff_prob[:,s]=mvgauss_prob(y,self.gibbs_mean[s,i-1],cov_inv,cov_det)  
                    prob[:,self.A_groups[s]]=eff_prob[:,[s]]
            for l in range(n_seqs): 
                # compute alpha variable
                forward_alpha,_=forward(
                                        prob[idx[l][0]:idx[l][1]],
                                        self.gibbs_A[i-1],
                                        self.gibbs_P[i-1]
                                        )
                # backward walk to sample from the state sequence
                backward_sample(
                                self.gibbs_A[i-1],
                                forward_alpha,
                                q[idx[l][0]:idx[l][1]],
                                transition_counter,
                                init_state_counter)
            # now, with a sample from the states (in q variable)
            # it is all quite similar to a gaussian mixture!
            for j in range(self.n_states):
                # sample from transition matrix
                self.gibbs_A[i,j]=np.random.dirichlet(alphas[j]+transition_counter[j])
            # make sure that the entries are zero!
            for e in self.A_zeros:
                self.gibbs_A[i,e[0],e[1]]=0.
            # renormalize in case we had to set something to zero
            self.gibbs_A[i]/=np.sum(self.gibbs_A[i],axis=1)[:,None]
            # sample from initial state distribution
            self.gibbs_P[i]=np.random.dirichlet(alpha_p+init_state_counter)   
            for j in range(self.eff_n_states):
                # basically, this is the code to sample from a multivariate
                # gaussian but constrained to observations where state=j
                idx_states=np.where(np.in1d(q,self.A_groups[j]))[0]
                # just sample from the prior!
                if idx_states.size==0:
                    self.gibbs_mean[j,i]=np.random.multivariate_normal(prev_mn[j],prev_Vn[j])
                    self.gibbs_cov[j,i]=invwishart.rvs(df=prev_vn[j],scale=prev_Sn[j])  
                else:
                    n_count=idx_states.size
                    x_=y[idx_states]
                    if self.irregular_obs:
                        y_mean_=np.zeros(self.p)
                        for pi in range(self.p):
                            aux=np.where(~np.isnan(x_[:,pi]))[0]
                            if aux.size!=0:
                                y_mean_[pi]=np.mean(x_[aux,pi])
                            else:
                                y_mean_[pi]=self.gibbs_mean[j,i-1][pi]
                    else:
                        y_mean_=np.mean(x_,axis=0)
                    # sample for mean
                    invC=np.linalg.inv(self.gibbs_cov[j,i-1])
                    Vn=np.linalg.inv(invV0+n_count*invC)
                    mn=np.dot(Vn,invV0m0+n_count*np.dot(invC,y_mean_))
                    prev_mn[j]=mn
                    prev_Vn[j]=Vn
                    if self.independent_vars:
                        Vn=np.diag(np.diag(Vn))
                    self.gibbs_mean[j,i]=np.random.multivariate_normal(mn,Vn)
                    # sample from cov
                    # get random k value (shrinkage value)
                    k=np.random.uniform(0,self.max_k)
                    n0=k*n_count
                    S0=n0*S0aux
                    v0=n0+self.p+1
                    vn=v0+n_count
                    if self.irregular_obs:
                        # we are forcing them to be independent if there are errors
                        St=np.zeros((self.p,self.p))
                        for pi in range(self.p):
                            aux=np.where(~np.isnan(x_[:,pi]))[0]
                            if aux.size!=0:
                                St[pi,pi]=np.sum(np.power(x_[aux,pi]-self.gibbs_mean[j,i][pi],2))
                            else:
                                St[pi,pi]=prev_Sn[j][pi,pi]
                    else:
                        St=np.dot((x_-self.gibbs_mean[j,i]).T,(x_-self.gibbs_mean[j,i]))
                    Sn=S0+St
                    prev_vn[j]=vn
                    prev_Sn[j]=Sn
                    if self.independent_vars:
                        Sn=np.diag(np.diag(Sn))
                    self.gibbs_cov[j,i]=invwishart.rvs(df=vn,scale=Sn)
                    if self.independent_vars:
                        self.gibbs_cov[j,i]=np.diag(np.diag(self.gibbs_cov[j,i])) 
        # burn observations
        self.gibbs_A=self.gibbs_A[-self.n_gibbs:]
        self.gibbs_P=self.gibbs_P[-self.n_gibbs:]
        self.gibbs_mean=self.gibbs_mean[:,-self.n_gibbs:,:]
        self.gibbs_cov=self.gibbs_cov[:,-self.n_gibbs:,:,:]
        self.A=np.mean(self.gibbs_A,axis=0)
        self.P=np.mean(self.gibbs_P,axis=0)
        self.states_mean=np.mean(self.gibbs_mean,axis=1)
        self.states_cov=np.mean(self.gibbs_cov,axis=1)
```

Now, we can try to use the class to estimate the parameters for the dataset generated.


```python
model=GaussianHMM(
            n_states=2,
            n_gibbs=500,
            A_zeros=[],
            A_groups=[],
            f_burn=0.1,
            max_k=0.25,
            pred_l=None,
            irregular_obs=True,
            independent_vars=True
            )      
model.estimate(xd)
model.view(True)
```

    ** GIBBS SAMPLER (HMM) **
    GROUPS
    - States 0 have the same emission
    - States 1 have the same emission
    INITIAL STATE PROBABILITY
    [0.96619567 0.03380433]
    


    
![png](/images/eco_gibbs/output_6_1.png)
    


    STATE TRANSITION MATRIX
    [[0.87741564 0.12258436]
     [0.11018713 0.88981287]]
    
    


    
![png](/images/eco_gibbs/output_6_3.png)
    


    STATE 1
    MEAN
    [-0.5184795   0.49408527]
    COVARIANCE
    [[0.08697658 0.        ]
     [0.         0.07827175]]
    
    


    
![png](/images/eco_gibbs/output_6_5.png)
    



    
![png](/images/eco_gibbs/output_6_6.png)
    


    STATE 2
    MEAN
    [ 0.49735388 -0.51020846]
    COVARIANCE
    [[0.08080218 0.        ]
     [0.         0.07998184]]
    
    


    
![png](/images/eco_gibbs/output_6_8.png)
    



    
![png](/images/eco_gibbs/output_6_9.png)
    


Appart from the cross-covariances the algorithm recovered the parameters. Now let us run a example with real data.

## US Growth and Inflation

As an example let us consider United States inflation, CPI, and a growth measure, the CLI (composite leading indicator). Although both have monthly frequencies they are generally released on different days and so we can apply the same idea as before to try to gain some insight into the data. The data represents monthly rates of changes (which will be translated into yearly changes).


```python
# EXAMPLE WITH REAL DATA
dataset=pd.read_csv('data_us.csv',index_col='Date',parse_dates=True)[['CLI','CPI']]
dataset.interpolate(method='linear').plot(grid=True)
plt.show()
```


    
![png](/images/eco_gibbs/output_8_0.png)
    



```python
tmp=dataset.copy(deep=True).dropna(how='all')
x=tmp.values
x*=12 # multiply by 12 to get a annual rate of change; easier to interpret
model=GaussianHMM(
            n_states=2,
            n_gibbs=500,
            A_zeros=[],
            A_groups=[],
            f_burn=0.1,
            max_k=0.25,
            pred_l=None,
            irregular_obs=True,
            independent_vars=True
            )      
model.estimate(x)
model.view(True,False)
```

    ** GIBBS SAMPLER (HMM) **
    GROUPS
    - States 0 have the same emission
    - States 1 have the same emission
    INITIAL STATE PROBABILITY
    [0.04807924 0.95192076]
    


    
![png](/images/eco_gibbs/output_9_1.png)
    


    STATE TRANSITION MATRIX
    [[0.96228879 0.03771121]
     [0.02082827 0.97917173]]
    
    


    
![png](/images/eco_gibbs/output_9_3.png)
    


    STATE 1
    MEAN
    [-0.00165463  0.05395486]
    


    
![png](/images/eco_gibbs/output_9_5.png)
    


    STATE 2
    MEAN
    [0.0002964  0.02632873]
    


    
![png](/images/eco_gibbs/output_9_7.png)
    


The model captured a state with moderate inflation and positive growth and another one with higher inflation with negative/smaller growth; also, the states are persistent. This kind of makes sense from what people believe these variables are related. Please notice that the choice to consider only two states was arbitrary and for purpose of simplicity of presentation. 

More interesting is to check if we can use this information to gain a more actionable insight. For example, learning the model in conjunction with a equity index fluctuations to verify if changes in the tradeable asset are related to changes in economic variables; also, this example now contains one variable (the equity index) with much more observations than the others (which are quite sparse in time in comparisson).


```python
# EXAMPLE WITH REAL DATA
dataset=pd.read_csv('data_us.csv',index_col='Date',parse_dates=True)[['CLI','CPI','Equity']]
tmp=dataset.copy(deep=True).dropna(how='all')
x=tmp.values 
# get a annual rate of change; easier to interpret
x[:,0]*=12 # CLI
x[:,1]*=12 # CPI
x[:,2]*=250 # Equity
model=GaussianHMM(
            n_states=2,
            n_gibbs=500,
            A_zeros=[],
            A_groups=[],
            f_burn=0.1,
            max_k=0.25,
            pred_l=None,
            irregular_obs=True,
            independent_vars=True
            )      
model.estimate(x)
model.view(True,False)
```

    ** GIBBS SAMPLER (HMM) **
    GROUPS
    - States 0 have the same emission
    - States 1 have the same emission
    INITIAL STATE PROBABILITY
    [0.81889116 0.18110884]
    


    
![png](/images/eco_gibbs/output_11_1.png)
    


    STATE TRANSITION MATRIX
    [[0.96918054 0.03081946]
     [0.16155005 0.83844995]]
    
    


    
![png](/images/eco_gibbs/output_11_3.png)
    


    STATE 1
    MEAN
    [0.00407773 0.0241238  0.15833691]
    


    
![png](/images/eco_gibbs/output_11_5.png)
    


    STATE 2
    MEAN
    [-0.02728537  0.08731298 -0.2339484 ]
    


    
![png](/images/eco_gibbs/output_11_7.png)
    


The result is interesting, we got the same dynamics as on the first example but the equity index shows a definite diferent behaviour in those states - positive and negative (although with lots of variance in the expectation of the mean). 

Of course this is not the evaluation of a predictive model but one easily adapts the procedure for that end - in that case one then makes a prediction for the next distribution but uses only the part relative to _investable_ series.

A practical model to deal with time series that contain different frequencies was developed and a method to estimate was shown; the intuition is that, there are some states that generate the observations and, within those states observations are independent - we can observe some time-clustering of moments (means, scales). This structure seems to fit quite well economic data and we can build tradeable models from it - without worrying too much about aligning data and/or other preprocessing that allow other techniques to be applied.
