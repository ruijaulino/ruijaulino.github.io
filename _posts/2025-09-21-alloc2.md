
# Allocation on optimal strategies

As we build many strategies we need to allocate resources between them. One could approach the problem as regular model allocation (now without predictors, just statistics on the returns) but it makes sense, in this case, to impose constraints as it can be difficult to get optimal leverage and overconcentration issues (also, this expose us more to estimation errors). 

This post contains some observations on this topic.




## Optimal strategies

As from other posts, given some features $z$, the optimal bet on return $x$ is given by:

$w(z) = \frac{1}{k}M_{x\|z}^{-1}\mu_{x\|z}$ 

where $M_{x\|z}$ is the conditional second non central moment and $\mu_{x\|z}$ is the conditional mean. Notice that a (constant) normalization factor was written as well to ensure that weights make sense from practical view point. These weights induces a strategy $s = w^T x$ with the following properties:

$\mathbb{E}\[s\] = \frac{1}{k} \mathbb{E}\_z[\mu_{x\|z}^T M_{x\|z}^{-1} \mu_{x\|z}\] = \frac{1}{k}g$

$\mathbb{E}\[s^2\] = \frac{1}{k^2} g$

The sharpe ratio is $\text{SR} = \sqrt{g}$ and the growth rate with this scheme is $G = g \left(\frac{1}{k} - \frac{1}{2k^2}\right)$.

Now we can ask what is a reasonable value for $k$; it is related to how $w$ is distributed, in particular to the scale of the distribution. Looking at the expression for $w$, the second non central moment (small biases/edge approximation):

$\mathbb{E}\[ww^T\] = \frac{1}{k^2}\mathbb{E}\_z\[M_{x\|z}^{-1} \mu\_{x\|z} \mu_{x\|z}^T M_{x\|z}^{-1}\] $

Since this is a matrix, a natural scalar measure of weight scale is the trace:

$\text{tr}(\mathbb{E}\[ww^T\]) = \frac{1}{k^2} \mathbb{E}\_z \[ \mu_{x\|z}^T M_{x\|z}^{-2} \mu\_{x\|z} \]$

Note that 

$\lambda_{\text{min}}(M_{x\|z}^{-1}) \mu_{x\|z}^T M_{x\|z}^{-1} \mu\_{x\|z} < \mu_{x\|z}^T M_{x\|z}^{-2} \mu\_{x\|z} < \lambda_{\text{max}}(M_{x\|z}^{-1}) \mu_{x\|z}^T M_{x\|z}^{-1} \mu\_{x\|z}$

so, the quantity $\mathbb{E}\_z \[ \mu_{x\|z}^T M_{x\|z}^{-2} \mu\_{x\|z} \]$ is of the same order as $g =  \mathbb{E}\_z[\mu_{x\|z}^T M_{x\|z}^{-1} \mu_{x\|z}\]$ multiplied by a typical eigenvalue of $M_{x\|z}^{-1}$ which we can proxy by $\text{tr}(M_{x\|z}^{-1})$. All constant and dimensional factors are absorbed into a calibration constant $m$.

Then, for $w$ to have order of magnitude of one, we can use

$k = m \text{SR} \sqrt{\text{tr}\left( M_{x}^{-1} \right)}$

A similar expression was already found for linear models (see post on it) but here it is generalized.


### Allocating in many optimal strategies

How should one proceed when we have many optimal strategies and we need to divide resources between them? Since correlations between strategies are a second order correction (last post), we have that the weight we give a strategy is:

$w_s = \frac{\mu_s}{\sigma_s^2} = k = m \text{SR} \sqrt{\text{tr}\left( M_{x}^{-1} \right)}$ 

Trivially, we need to allocate a value equal to it's $k$ which means that we allocate to get optimal leverage (this result is obvious) but, given how $k$ is calculated we can derive other conclusions that may help to derive and/or justify heuristics.

For example, consider a strategy $s_1$ on a single asset and a strategy $s_2$ on $p$ assets (all assets with the same volatility and all strategies with the same sharpe): we should put more money on the strategy with fewer assets. Probably it is easier to achieve a higher sharpe when many assets are considered so this observation may not have much practical insight.

If we have many strategies with similar sharpes and same number of assets then what matters is the volatility of assets. Same risk adjusted performance with lower vol yields a larger allocation. The problem is this may not be practical because we cannot lever and we are exposing ourselfs to estimation errors. On the other hand, if volatility is the same, we can guide though sharpe ratio.


It seems reasonable that we should consider a more balanced portfolio when we are dealing with strategies (expected values should be already positive and the statistics we have from them should be already removed from modelling problems).


## Penalized Optimal allocation

The natural optimal condition of maximal growth rate $G = w^T\mu - \frac{1}{2}w^T C w$ can be modified to accomodate a penalty if the solution is too concentrated: 

$\bar{G} = G - \frac{\beta}{2}w^Tw = w^T\mu - \frac{1}{2}w^T D w$

with $D = C + \beta I$. When $\beta$ is higher we put a higher penalty into concentrated weights. Since it is impossible to make _physical_ sense of this parameter, we can 
investigate how the solution behaves when it changes and develop heuristics. Given our focus is in strategies allocation, we will focus on diagonal covariances $C$.


#### Unconstrained solution

The problem has a trivial solution:

$w_i = \frac{\mu_i}{\sigma_i^2 + \beta}$

looking at $\frac{1}{\sigma_i^2 + \beta} = \frac{1}{\sigma_i^2} \frac{1}{1+\frac{\beta}{\sigma_i^2}}$, the expansion for large $z$:

$\frac{1}{1+z} \approx \frac{1}{z} - \frac{1}{z^2}$

this means that, for large $\frac{\beta}{\sigma_i^2}$, the behaviour is dominated by:

$\frac{1}{\sigma_i^2 + \beta} \approx \frac{1}{\beta} - \frac{\sigma_i^2}{\beta^2}$

with that, in the limit of large concentration penalty:

$w_i \rightarrow \mu_i \left( \frac{1}{\beta} - \frac{\sigma_i^2}{\beta^2} \right)$

and so the weights are tilted to the strategies with higher mean with a higher order correction penalizing higher variance.

#### Constrained solution

A more realistic case is when we introduce a budget constraint like $\sum_i w_i = 1$. So we want to maximize $G = w^T \mu - \frac{1}{2}w^T D w$ subject to $w^T \vec{1} = 1$. The lagrangian for this problem becomes $L = w^T \mu - \frac{1}{2}w^T D w - \lambda (w^T \vec{1} - 1)$ and the stationary point is easily calculated as

$w = D^{-1} \left( \mu - \lambda \vec{1}\right)$

The constraint implies that $\lambda = \frac{\mu^T D^{-1} \vec{1} - 1}{\vec{1}^T D^{-1} \vec{1}}$.

Diagonal covariance implies

$w_i = \frac{\mu_i - \lambda}{\sigma_i^2 + \beta}$

and

$\lambda = \frac{\sum_j \frac{\mu_j}{\sigma_j^2 + \beta} - 1}{\sum_j \frac{1}{\sigma_j^2 + \beta}}$

using the same expansion for large $\beta$ and working out the terms we can compute the approximation

$w_i \approx \frac{1}{n} + \frac{1}{\beta}\left( \mu_i - \bar{\mu} + \frac{\bar{\sigma^2} - \sigma_i^2}{n}\right)$

from where we can see that the solution is the equal weight portfolio with a correction putting more weights into the strategies with larger mean and lower variance (although the variance term is smaller). This is an interesting approximation for the allocation under constraints as means and variances tend to be quite small numbers. 


## Concavity with respect to another allocation

Given that we can create heuristics (for example, just use equal weight), it becomes useful to understand what we lose in terms of growth by doing that. Let's see what
happens when we use combinations of solutions. Again, $G = w^T\mu - \frac{1}{2} w^T C w$ and the optimal weight is $w^* = C^{-1} \mu$. 

Let $w$ be a convex combination of the optimal allocation and another weight vector $q$, $w = \alpha w^* + (1-\alpha) q$. 
If $\alpha$ goes to $1\[0\]$ then $w \rightarrow w^*\[q\]$. How does the growth rate behave when we change $\alpha$? 

We have

$\frac{\partial G}{\partial \alpha} = \left( \mu - Cw\right)^T \left( w^* - q \right)$

and 

$\frac{\partial^2 G}{\partial \alpha^2} = - \left( w^* - q \right)^T C \left( w^* - q \right)$

since $C$ is positive definite we can see that the growth rate is _concave_ to $\alpha$: moving the weight torwards a target weight $q$ does not 
reduce the growth rate in the same proportion, it reduces less and so, considering $q$ a equal weight vector, we can blend a equal weight 
allocation (reducing concentration as we saw) without loosing a proportional amount in growth which is a desirable property and shows that the heuristics has 
optimal-like properties.



# Improving strategies or finding more?

Should we focus in finding more strategies or improving the ones we already have? After we allocate into a bunch of strategies (in a optimal way, no heuristics) we
can write the sharpe ratio of the portfolio of strategies as (it is the same logic as the results before):

$SR = \sqrt{\mu_s^T M_s^{-1} \mu_s} = \sqrt{g}$

where $\mu_s$ and $M_s$ are the moments of the strategies. Assuming they are uncorrelated and the second non central moment is similar to the variance (for more detail on the general expression check the bet with models post):

$SR \approx \sqrt{\sum_s \frac{\mu_s^2}{\sigma_s^2}}$

where we sum over all strategies. Using a linear model baseline (univariate prediction of univariate returns, no biases and small correlation for the approximations used), $x = bz + \epsilon$, $z \sim N(0, \phi^2)$, $\epsilon \sim N(0, \sigma^2)$ 
we have that $\mu_s = \frac{b^2 \phi^2}{\sigma^2} \approx \sigma_s^2$. Then 

$\text{SR}^2 = n \bar{\rho^2}$

with $\bar{\rho^2} = \frac{1}{n} \sum_j \rho_j^2$ and $\rho_j = \frac{b_i \phi_i}{\sigma_i}$. 

Let us consider the correlation of features with targets all equal to $\rho$ then:

$\text{SR}^2 = n \rho^2$

If we have many mediocre strategies as we gather more we improve the result. Another route is to improve the features/models and have a larger correlation with the 
targets (here, other effects kick in as the predictive covariance now is different from the unconditional one! but let's ignore as it 
does not change the conclusions); this could be attained by improving the features to get a higher correlation with the target. What is the most effective method?

$\frac{\partial \text{SR}}{\partial \rho} = \sqrt{n} > 0$

$\frac{\partial \text{SR}}{\partial n} = \frac{\|\rho\|}{2\sqrt{n}} > 0$

we can see that

$\frac{\partial \text{SR}}{\partial \rho} > \frac{\partial \text{SR}}{\partial n} \rightarrow 2 n > \|\rho\| $

which is always true. So it is always more effective (in the sense that it is easier to move the neddle in the final sharpe) to improve the models rather than finding new ones. 
One can argue that it is more feasible to find a new mediocre strategy than to improve a existent one (also, 
improving/changing strategies can induce overfitting as data is reutilized again and again), but, nevertheless, this simple result can help into thinking about the research process. 


