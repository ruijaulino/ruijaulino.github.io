# LOOCV Strategy Performance Estimator for Linear Models

Consider the problem where we seek to predict returns $y$ as a linear function of various features $x$. One of the challenges in this process is to evaluate how well our model will perform on unseen data. A common approach is cross-validation and, a particular procedure, is Leave-One-Out Cross-Validation (LOOCV), where we train the model on all but one data point and then test on that excluded point. This process, when repeated for all data points, provides a estimate of out-of-sample performance. Here, we discuss an efficient method to estimate LOOCV for linear models, specifically targeting the evaluation of trading strategies. 

## Model Setup

Let us consider a model where financial returns $y$ are a linear function of $q$ features $x$:

$y = b_0 + b_1 x_1 + b_2 x_2 + \cdots + b_q x_q + \epsilon$

where $\epsilon \sim N(0,\sigma^2)$. In matrix notation, we can express this as:

$Y = Xb + \mathcal{E}$

where:
- $Y$ is an $n \times 1$ vector of observed returns,
- $\mathcal{E}$ is an $n \times 1$ vector of noise terms,
- $b$ is a $q \times 1$ vector of coefficients (weights),
- $X$ is an $n \times q$ matrix of features, where $q$ is the number of features (e.g., in a simple linear regression with an intercept, $q = 2$).

Given the data, the least squares estimator for $b$ is:

$\hat{b} = \left( X^TX \right)^{-1} X^T Y$

The in-sample prediction of $Y$ is:

$\hat{Y} = X \hat{b} = X \left( X^TX \right)^{-1} X^T Y = H Y$

where $H$ is the $n \times n$ "hat" matrix. Let $h$ be the vector containing the diagonal entries of $H$; each element can be expressed as:

$h_i = x_i^T \left( X^TX \right)^{-1} x_i$

The noise variance can be estimated by the variance of the residuals:

$\hat{\sigma^2} = \text{Var}\left(y - \hat{y}\right)$

### Optimal Bet Size

In a trading model, the optimal amount to invest (weight) based on the model's predictions is:

$w^* = \frac{\mu_{y\|x}}{M_{y\|x}}$

where:
- $\mu_{y\|x} = x^T\hat{b}$ is the conditional mean of $y$ given $x$,
- $M_{y\|x} = \hat{\sigma^2} + \mu_{y\|x}^2$ is the second non-central moment of $y$ given $x$.

We can make an approximation assuming returns are small compared to their scale, such that $M_{y\|x} \approx \hat{\sigma^2}$. This simplification allows us to estimate the optimal weight $w^*$ efficiently.

## Fast Leave-One-Out Cross-Validation

LOOCV involves training the model on all data points except one and testing on the excluded point. We repeat this process for each data point to evaluate out-of-sample performance. Let $\tilde{X}\_{i}$ and $\tilde{Y}\_{i}$ be the matrices $X$ and $Y$ excluding the $i$-th row, respectively.

We can write:

$\tilde{X}\_{i}^T \tilde{X}\_{i} = X^TX - x_ix_i^T$

The inverse of this matrix is given by:

$\left(\tilde{X}\_{i}^T \tilde{X}\_{i}\right)^{-1} = \left(X^TX - x_ix_i^T\right)^{-1} = \left(X^TX\right)^{-1} + \frac{\left(X^TX\right)^{-1} x_ix_i^T \left(X^TX\right)^{-1}}{1 - h_i}$

Similarly, we have:

$\tilde{X}\_{i}^T \tilde{Y}\_{i} = X^TY - x_iy_i$

Using these expressions, the regression coefficients without observation $i$ can be computed as:

$\hat{b_i} = \hat{b} - \left( X^TX \right)^{-1} x_i \frac{y_i - x_i^T \hat{b}}{1 - h_i}$

The out-of-sample performance of the trading strategy, when observation $i$ is excluded, can be approximated by:

$s_i \propto y_i x_i^T\hat{b}_i$

Including transaction costs $f$ (as a percentage), the performance adjusts to:

$s_i \propto \left(y_i - f\right) x_i^T\hat{b}_i$

We made an approximation assuming the conditional variance remains relatively constant when excluding points. By substituting $\hat{b_i}$, the performance metric can be expressed as:

$s_i \propto \left(y_i - f \right) \frac{1}{1 - h_i} \left( x_i^T \hat{b} - y_i h_i \right)$

This indicates a penalization to in-sample performance, which is intuitive as it reflects the uncertainty associated with the prediction. This formula allows us to efficiently estimate out-of-sample performance without repeatedly re-fitting the model.

### Strategy Statistics

To evaluate the overall strategy, we compute:

$\mu_s = \frac{1}{n}\sum_i s_i, \quad \sigma_s = \frac{1}{n} \sum_i \left(s_i - \mu_s \right)^2$

The Sharpe ratio, which measures risk-adjusted returns, can be used to compare strategies:

$\text{Sharpe Ratio} = \frac{\mu_s}{\sigma_s}$

Properly adjusting for time scales, this allows for efficient scanning over multiple strategies.



## Efficient Computation Using QR Decomposition

When assessing multiple strategies, it becomes important to optimize calculations. One approach is using QR decomposition:

$X = QR$

where $Q$ is an orthogonal $\left(Q^TQ = I\right)$ matrix and $R$ is an upper triangular matrix.

To solve for the coefficients, we can consider the solution to the linear system (instead of inverting the matrix. This tends to be more stable numerically but not properly an optimization):

$\left( X^T X\right)^{-1} \hat{b} = X^T Y$

which can be written as:

$R \hat{b} = Q^T Y$

A large part of the calculation is spent on computing the full matrix $H$:

$H = X \left(X^TX\right)^{-1}X^T = QR\left(R^T R\right)^{-1}R^TQ^T = QQ^T$

Since we only need the diagonals, the vector $h$ can be determined without the full matrix multiplication:

$h_i = \sum_j Q_{ij}^2$


Let us test the formulas with a numerical example. Comments on the code describe the procedure.


```python
import numpy as np
import time
```


```python
# generate data 
# y depends on a single feature x 
n = 10000
a = 0.1
b = 0.5
scale = 1
x = np.random.normal(0, 2, n)
y = a + b*x + np.random.normal(0, scale, n)
```


```python
# function to compute the linear regression and evaluate the strategy with LOOCV

def linreg(x, y, calc_s:bool = False, fee:float = 0., use_qr:bool = True):
    assert x.ndim == 1, "x must be a vector"
    assert y.ndim == 1, "y must be a vector"
    assert x.size == y.size, "x and y must have the same length"
    n = x.size
    X = np.column_stack((np.ones(n), x))
    if calc_s:
        if use_qr:
            # Compute b using QR decomposition
            Q, R = np.linalg.qr(X)
            b = np.linalg.solve(R, Q.T @ y)
            h = np.sum(Q**2, axis=1)
            s = (y-fee)*(X @ b - y*h)/(1-h)
        else:
            tmp = np.linalg.pinv(X.T @ X) @ X.T
            b = tmp @ y
            h = np.diag(X @ tmp)
            s = (y-fee)*(X @ b - y*h)/(1-h)
    else:
        b = np.linalg.pinv(X.T @ X) @ X.T @ y
        s = None
    return b, s

# function to compute the LOOCV with brute force
def brute_loocv_performance(x, y):
    assert x.ndim == 1, "x must be a vector"
    assert y.ndim == 1, "y must be a vector"
    assert x.size == y.size, "x and y must have the same length"
    n = x.size
    idx = np.arange(n)
    s = np.zeros(n)
    for i in range(x.size):
        train_idx = np.delete(idx, i)
        # compute without the strategy
        b, _ = linreg(x[train_idx], y[train_idx])
        s[i] = y[i]*(b[0]+b[1]*x[i])
    return s

t = time.time()
b, s1 = linreg(x, y, True, use_qr = True)
t1 = time.time() - t
print('Optimized formula time [s]: ', t1)

t = time.time()
b, s2 = linreg(x, y, True, use_qr = False)
t2 = time.time() - t
print('Naive time [s]: ', t2)

t = time.time()
s3 = brute_loocv_performance(x, y)
t3 = time.time() - t 

print('Brute force time [s]: ', t3)
print()
print('Strategy Sharpes [all equal as expected]')
print('Optimized: ', np.mean(s1)/np.std(s1))
print('Naive', np.mean(s2)/np.std(s2))
print('Brute force: ', np.mean(s3)/np.std(s3))
print()
print('Speed up [optimized version is the reference]')
print('Naive: ', t2/t1)
print('Brute force: ', t3/t1)
```

    Optimized formula time [s]:  0.001978635787963867
    Naive time [s]:  0.4080390930175781
    Brute force time [s]:  3.8069987297058105
    
    Strategy Sharpes [all equal as expected]
    Optimized:  0.5861576333832925
    Naive 0.5861576333832926
    Brute force:  0.5861576333832924
    
    Speed up [optimized version is the reference]
    Naive:  206.22243643812507
    Brute force:  1924.0522954572841
    
