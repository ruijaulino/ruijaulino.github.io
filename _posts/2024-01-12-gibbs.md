# Using a gibbs sampler to fit models

Consider that $p(x,y)$ is difficult to sample from but we can sample easily from $p(x\|y)$ and $p(y\|x)$.

The Gibbs sampler proceeds by repeating:
- sample $x_i$ from $p(x\|y_{i-1})$
- sample $y_i$ from $p(y\|x_{i})$

An interesting use of this is to sample from the posterior distribution of model parameters, $p(\theta\|D)$. Equiped with samples from this distribution we can compute expected value of parameters and check how those parameter can vary.

##### Example (Multivariate Gaussian)

Let $x \sim N(\mu,C)$ and assume we have an algorithm to sample from a univariate Gaussian (should be easy to program such algorithm given we are equiped with a uniform random number generator). The distribution of $x_i\|x_1,x_2,\cdots$ is well defined and is a univariate Gaussian. Following the previous steps should be easy (note that this is not the most efficient way to generate samples from this distribution but illustrates the idea).


```python
# Parameters
n=1000 # number of samples to generate
# Distribution parameters
mean=np.array([0.5,-0.5]) # mean vector
scale_1=0.1 # scale of variable 1
scale_2=0.2 # scale of variable 2
rho=0.5 # correlation
# build covariance from parameters
cov=np.array([
            [scale_1*scale_1,rho*scale_1*scale_2],
            [rho*scale_1*scale_2,scale_2*scale_2]
            ])
# use numpy to generate samples to compare to the implemented sampler
np_gauss_samples=np.random.multivariate_normal(mean,cov,n)
# Implement the gibbs sampler
add=0.1 # add some extra points to burn later 
# initialize array to store the generated samples
gibbs_gauss_samples=np.zeros((int(n*(1+add)),2))
# parameters for the conditional distribution
# of each variable
scale_12=np.sqrt(scale_1*scale_1*(1-rho*rho))
scale_21=np.sqrt(scale_2*scale_2*(1-rho*rho))
for i in range(1,gibbs_gauss_samples.shape[0]):
    mean_12=mean[0]+(gibbs_gauss_samples[i-1,1]-mean[1])*rho*scale_1/scale_2    
    gibbs_gauss_samples[i,0]=np.random.normal(mean_12,scale_12)
    mean_21=mean[1]+(gibbs_gauss_samples[i,0]-mean[0])*rho*scale_2/scale_1  
    gibbs_gauss_samples[i,1]=np.random.normal(mean_21,scale_21)  
gibbs_gauss_samples=gibbs_gauss_samples[-n:] # burn initial samples
plt.plot(np_gauss_samples[:,0],np_gauss_samples[:,1],'.',color='g',label='From numpy')
plt.plot(gibbs_gauss_samples[:,0],gibbs_gauss_samples[:,1],'.',color='r',label='Implemented sampler')
plt.grid(True)
plt.legend()
plt.show()
```


    
![png](/images/gibbs/output_3_0.png)
    

The generated samples overlap quite well with the ones generated with the numpy function; this example is a good starting point to understand the idea.


Now we will go on by showing how to sample from the posterior of parameters for several different models.

## Gaussian 

Consider the data $D = \{x_1,x_2,\cdots,x_n\}$ as independent observations from a Gaussian, $p(x)\sim N(x\|\mu,C)$. We are interested in finding the parameters $\mu$ and $C$ that are compatible with the observations. In other words, what is $p(\mu,C\|D)$?

We can write

$p(\mu,C\|D) \propto p(D\|\mu,C)p(\mu,C)$

If we consider the prior as

$p(\mu,C)=p(\mu)p(C)=N(\mu\|m,V)IW(C\|\nu,S)$

where $IW(\cdot)$ is the Inverse-Wishart distribution, we can see that this is a semi-conjugate prior because (independence assumptions)

$p(\mu\|C,D) \propto p(D\|\mu,C)p(\mu)$

and 

$p(C\|\mu,D) \propto p(D\|\mu,C)p(C)$

have the same functional form of the assumed prior. In other words, given that $C$ \[ $\mu$ \] is fixed, the prior for $\mu$ \[ $C$ \] is conjugate. Following the previous procedure we can apply the Gibbs sampler in a simple way. 


First, note that: 

$p(D\|\mu,C)=\Pi_i N(x_i\|\mu,C) = (2\pi)^{-\frac{nd}{2}} \|C\|^{-\frac{n}{2}} \exp \left[ -\frac{1}{2} \sum_i (x_i-\mu)^T C^{-1} (x_i-\mu) \right]$

We can write the term inside the exponential as

$\sum_i (x_i-\mu)^T C^{-1} (x_i-\mu) = \text{Tr}\left( C^{-1} S_{\bar{x}} \right) + n (\bar{x}-\mu)^T C^{-1} (\bar{x}-\mu)$

with $S_{\bar{x}}=\sum_i \left(x_i-\bar{x}\right)\left(x_i-\bar{x}\right)^T$ and $\bar{x}=\frac{1}{n}\sum_i x_i$

#### Inference for mean

Let the prior be $p(\mu)=N(\mu\|m_0,V_0)$. The posterior $p(\mu\|D)\propto p(D\|\mu)p(\mu)$ can be worked out to give (recall that the covariance, $C$, is assumed known)

$p(\mu\|D) \sim N(\mu\|m_n,V_n)$

with $m_n=V_n\left( V_0^{-1}m_0+nC^{-1}\bar{x} \right)$ and $V_n^{-1} = V_0^{-1} + n C^{-1}$


#### Inference for covariance

Let the prior for the covariance be the Inverse-Wishart distribution, $p(C) \sim IW(C\|\nu_0,S_0)$. The posterior $p(C\|D)\propto p(D\|C)p(C)$ can be worked out to give (recall that the mean, $\mu$, is assumed known)

$p(C\|D) \sim IW(C\|\nu_n,S_n)$

with $\nu_n = \nu_0 + n$ and $S_n = S_0+S_{\bar{x}}$

With these two expressions we can sample easily from the posterior distribution of the Gaussian: just sample from $p(\mu\|D,C)$ and $p(C\|D,\mu)$ with the previous sampled value of $C$ and $\mu$ respectivelly.

Let us illustrate this with a numerical experiment.



```python
class Gaussian(object):
    def __init__(self,n_gibbs=500,f_burn=0.1,max_k=0.25,names=None):
        self.f_burn=f_burn
        self.n_gibbs=n_gibbs
        self.max_k=max_k
        self.names=names        
        # real number of samples to simulate
        self.n_gibbs_sim=int(self.n_gibbs*(1+self.f_burn))
        self.p=1
        # to be calculated!
        self.gibbs_cov=None
        self.gibbs_mean=None
        self.mean=None
        self.cov=None
        self.cov_inv=None

    def view(self,plot_hist=True):
        if self.names is None:
            self.names=["x%s"%(i+1) for i in range(self.p)]
        if len(self.names)!=self.p:
            self.names=["x%s"%(i+1) for i in range(self.p)]					
        print('** GIBBS SAMPLER (G) **')
        print('MEAN')
        print(self.mean)
        print('COVARIANCE')
        print(self.cov)
        if self.gibbs_mean is not None:
            if plot_hist:
                for i in range(self.p):
                    plt.hist(
                            self.gibbs_mean[:,i],
                            density=True,
                            alpha=0.5,
                            label='Mean %s'%(self.names[i])
                            )
                plt.legend()
                plt.grid(True)
                plt.show()
        if self.gibbs_cov is not None:
            if plot_hist:
                for i in range(self.p):
                    for j in range(i,self.p):
                        plt.hist(
                                self.gibbs_cov[:,i,j],
                                density=True,
                                alpha=0.5,
                                label='Cov(%s,%s)'%(self.names[i],self.names[j])
                                )
                plt.legend()
                plt.grid(True)
                plt.show()

    def estimate(self,y,**kwargs):
        assert y.ndim==2,"y must be a matrix"
        n=y.shape[0]
        self.p=y.shape[1]
        # compute data covariance
        c=np.cov(y.T)
        if c.ndim==0:
            c=np.array([[c]])
        c_diag=np.diag(np.diag(c))
        # prior parameters
        m0=np.zeros(self.p) # mean prior center
        V0=c_diag.copy() # mean prior covariance
        S0aux=c_diag.copy() # covariance prior scale (to be multiplied later)
        # precalc
        y_mean=np.mean(y,axis=0)
        invV0=np.linalg.inv(V0)
        invV0m0=np.dot(invV0,m0)
        # initialize containers
        self.gibbs_cov=np.zeros((self.n_gibbs_sim,self.p,self.p))
        self.gibbs_mean=np.zeros((self.n_gibbs_sim,self.p))
        # initialize cov
        self.gibbs_cov[0]=c	
        # sample
        for i in range(1,self.n_gibbs_sim):
            # sample for mean
            invC=np.linalg.inv(self.gibbs_cov[i-1])
            Vn=np.linalg.inv(invV0+n*invC)
            mn=np.dot(Vn,invV0m0+n*np.dot(invC,y_mean))
            self.gibbs_mean[i]=np.random.multivariate_normal(mn,Vn)
            # get random k value (shrinkage value)
            # instead of assuming a fixed value for v0
            # we consider it a multiple of the number of observations
            k=np.random.uniform(0,self.max_k)
            n0=k*n
            S0=n0*S0aux
            v0=n0+self.p+1
            vn=v0+n
            St=np.dot((y-self.gibbs_mean[i]).T,(y-self.gibbs_mean[i]))
            Sn=S0+St
            self.gibbs_cov[i]=invwishart.rvs(df=vn,scale=Sn) 
        self.gibbs_mean=self.gibbs_mean[-self.n_gibbs:]
        self.gibbs_cov=self.gibbs_cov[-self.n_gibbs:]
        self.mean=np.mean(self.gibbs_mean,axis=0)
        self.cov=np.mean(self.gibbs_cov,axis=0)

# Now let us generate some observations from a Multivariate Gaussian
# Parameters
n=1000 # number of samples to generate
# Distribution parameters
mean=np.array([0.5,-0.5]) # mean vector
scale_1=0.1 # scale of variable 1
scale_2=0.2 # scale of variable 2
rho=0.5 # correlation
# build covariance from parameters
cov=np.array([
            [scale_1*scale_1,rho*scale_1*scale_2],
            [rho*scale_1*scale_2,scale_2*scale_2]
            ])
print('** ORIGINAL PARAMETERS **')
print('MEAN')
print(mean)
print('COVARIANCE')
print(cov)
print('')
# use numpy to generate samples to compare to the implemented sampler
samples=np.random.multivariate_normal(mean,cov,n)
g=Gaussian(n_gibbs=2000)
g.estimate(samples)
g.view(True)
```

    ** ORIGINAL PARAMETERS **
    MEAN
    [ 0.5 -0.5]
    COVARIANCE
    [[0.01 0.01]
     [0.01 0.04]]
    
    ** GIBBS SAMPLER (G) **
    MEAN
    [ 0.49863538 -0.49354752]
    COVARIANCE
    [[0.01020059 0.00853414]
     [0.00853414 0.03993451]]
    


    
![png](/images/gibbs/output_5_1.png)
    



    
![png](/images/gibbs/output_5_2.png)
    


## Gaussian Mixture

Now this is more interesting because, opposite to the previous case, we cannot find a closed expession to fit the model and so this is useful to make the fit in a more robust way (for example, the EM algorithm is quite sensible to the initialization).

If $x$ is generated from a Gaussian Mixture then $p(x) = \sum_j \pi_j N(x\|\mu_j,C_j)$ (with the weights $\pi_j$ summing to one).

Consider also a unobserved variable $z$ which represents the state that generated emission x (it is a vector with the same number of observations as the data). 

We want to sample from the joint distribution of the parameters and a way to achieve this is to be able to sample from each conditional distribution (Gibbs sampler idea); note that, for each parameter $\theta_k$, the distribution $p(\theta_k\|\theta_{/k})$ is proportional to the joint $p(\theta_k,\theta_{/k})$.

The joint distribution can be written as

$p(\pi,\mu,C,z,D) = p(D\|\pi,\mu,C,z)p(z\|\pi,\mu,C)p(\mu,C,\pi)$

Now, given that we assumed the existence of the $z$ variables, the data $D$ does not depend on $\pi$ anymore ($\pi$ is used to generate $z$ that is used to generate $x$); the variables $z$ only depends on $\pi$ (think, for example, in a program to simulate observations from the mixture: first we sample a state $z$ with parameters $\pi$ and then we generate the observation accordingly). Also, as an assumption, let the distribution of parameters be independent $p(\mu,C,\pi)=p(\mu)p(C)p(\pi)$.

The final model for the joint is:

$p(\pi,\mu,C,z,D) = p(D\|\mu,C,z)p(z\|\pi)p(\mu)p(C)p(\pi)$

We can work the first term and write it as

$p(D\|\pi,\mu,C,z) = \Pi_i \left[ \Pi_j N(x_i\|\mu_j,C_j)^{I(z_i=j)} \right]$

The second term is simply

$p(z_i=j\|\pi)=\pi_j$

Their multiplication becomes

$p(D\|\pi,\mu,C,z)p(z\|\pi) = \Pi_i \left[ \Pi_j \pi_j N(x_i\|\mu_j,C_j)^{I(z_i=j)} \right]$

Putting it all together and noting that an appropriate prior for $\pi$ is a Dirichet distribution

$p(\pi,\mu,C,z,D) = \Pi_i \left[ \Pi_j \pi_j N(x_i\|\mu_j,C_j)^{I(z_i=j)} \right] Dir(\pi\|\alpha) \Pi_k\left[ N(\mu_k\|m_0,V_0) IW(C_k\|\nu_0,S_0) \right]$

For every parameter, the conditional is proportional to the joint. For example:

$p(z\|D,\mu,C,\pi) \propto p(\pi,\mu,C,z,D)$

Keeping the terms where $z$ shows up:

$p(z\|D,\mu,C,\pi) \propto p(D\|\mu,C,z)p(z\|\pi)$

Looking at it we can write:

$p(z_i=j\|x_i,\mu,C,\pi) \propto \pi_jN(x_i\|\mu_j,C_j)$

Doing the same for $\pi$ we get:

$p(\pi\|z) = Dir(\alpha_j+\sum_i I(z_i=j))$

For the states gaussian parameters this becomes quite similar to the previous section, but now confined to the observations where $z$ belongs to that state.

$p(\mu_j\|C_j,z,D) = N(\mu_j\|m_j,V_j)$

with $V_j^{-1} = V_0^{-1} + n_j C_j^{-1}$, $m_j = V_j \left( n_j C_j^{-1} \bar{x}_j + V_0^{-1} m_0   \right)$, $n_j=\sum_iI(z_i=j)$ and $\bar{x}_j = \frac{1}{n_j}\sum_i x_iI(z_i=j)$.

For the covariance the story is similar

$p(C_j\|\mu_j,z,D) = IW(C_j\|\nu_j,S_j)$

with $\nu_j = \nu_0 + n_j$ and $S_j = S_0 + \sum_i \left( I(z_i=j) (x_i-\mu_j)(x_i-\mu_j)^T\right)$

With this we can generate a Gibbs sampler.
The following code illustrates this




```python
PROB_NUM_PREC=1e-8
def mvgauss_prob(x,mean,cov_inv,cov_det):
    '''
    x: numpy (n,p) array 
        each row is a joint observation
    mean: numpy (p,) array with the location parameter
    cov_inv: numpy(p,p) array with the inverse covariance 
            can be computed with np.linalg.inv(cov)
    cov_det: scalar with the determinant of the covariance matrix 
            can be computed with np.linalg.det(cov)
    returns:
        out: numpy(n,) array where each value out[i] is the 
                probability of observation x[i] 
    '''
    k=mean.size # number of variables
    x_c=x-mean # center x
    # vectorized computation     
    out=np.exp(-0.5*np.sum(x_c*np.dot(x_c,cov_inv),axis=1))/np.sqrt(np.power(2*np.pi,k)*cov_det)
    out[out<PROB_NUM_PREC]=PROB_NUM_PREC
    return out

class GaussianMixture(object):
    def __init__(self,n_states=2,n_gibbs=1000,f_burn=0.1,max_k=0.25,names=None):
        self.n_states=n_states
        self.f_burn=f_burn
        self.n_gibbs=n_gibbs
        self.max_k=max_k
        self.names=names
        # real number of samples to simulate
        self.n_gibbs_sim=int(self.n_gibbs*(1+self.f_burn))
        self.p=1
        self.gibbs_cov=None
        self.gibbs_mean=None

    def view(self,plot_hist=True):
        print('** GIBBS SAMPLER (GMM) **')
        if self.names is None:
            self.names=['x%s'%(i+1) for i in range(self.p)]
        for j in range(self.n_states):
            print('STATE %s'%(j+1))
            print('PROB: ', self.states_pi[j])
            print('MEAN')
            print(self.states_mean[j])
            print('COVARIANCE')
            print(self.states_cov[j])
            print()		
            if plot_hist:
                if self.gibbs_mean is not None:
                    for i in range(self.p):
                        plt.hist(
                                self.gibbs_mean[j,:,i],
                                density=True,
                                alpha=0.5,
                                label='Mean %s'%(self.names[i])
                                )
                    plt.legend()
                    plt.grid(True)
                    plt.show()
                if self.gibbs_cov is not None:
                    for i in range(self.p):
                        for q in range(i,self.p):
                            plt.hist(
                                    self.gibbs_cov[j,:,i,q],
                                    density=True,
                                    alpha=0.5,
                                    label='Cov(%s,%s)'%(self.names[i],self.names[q])
                                    )
                    plt.legend()
                    plt.grid(True)
                    plt.show()

    def estimate(self,y,**kwargs):
        # Gibbs sampler
        assert y.ndim==2,"x must be a matrix"
        n=y.shape[0]
        self.p=y.shape[1]
        # compute data covariance
        c=np.cov(y.T)
        if c.ndim==0:
            c=np.array([[c]])
        c_diag=np.diag(np.diag(c))
        # prior parameters
        m0=np.zeros(self.p) # mean prior center
        V0=c_diag.copy() # mean prior covariance
        S0aux=c_diag.copy() # covariance prior scale (to be multiplied later)
        alpha0=self.n_states
        # precalc
        x_mean=np.mean(y,axis=0)
        invV0=np.linalg.inv(V0)
        invV0m0=np.dot(invV0,m0)
        # initialize containers
        self.gibbs_pi=np.zeros((self.n_states,self.n_gibbs_sim))
        self.gibbs_cov=np.zeros((self.n_states,self.n_gibbs_sim,self.p,self.p))
        self.gibbs_mean=np.zeros((self.n_states,self.n_gibbs_sim,self.p))
        aux=np.zeros((n,self.n_states))
        # initialize cov
        for i in range(self.n_states):
            self.gibbs_cov[i,0]=c
        # others
        possible_states=np.arange(self.n_states)
        c=np.random.choice(possible_states,n)
        n_count=np.zeros(self.n_states)
        # previous used parameters to sample when there are not observations
        # on that state
        prev_mn=np.zeros((self.n_states,self.p))
        prev_Vn=np.zeros((self.n_states,self.p,self.p))
        prev_vn=np.zeros(self.n_states)
        prev_Sn=np.zeros((self.n_states,self.p,self.p))
        for j in range(self.n_states):
            prev_mn[j]=m0
            prev_Vn[j]=V0
            prev_vn[j]=self.p+1+1
            prev_Sn[j]=S0aux
        # sample
        for i in range(1,self.n_gibbs_sim):
            for j in range(self.n_states):
                # basically, this is the code to sample from a multivariate
                # gaussian but constrained to observations where state=j
                # check previous section and compare
                idx=np.where(c==j)[0]
                # just sample from the prior!
                if idx.size==0:
                    self.gibbs_mean[j,i]=np.random.multivariate_normal(prev_mn[j],prev_Vn[j])
                    self.gibbs_cov[j,i]=invwishart.rvs(df=prev_vn[j],scale=prev_Sn[j])  
                else:
                    n_count[j]=idx.size
                    x_=y[idx]
                    x_mean=np.mean(x_,axis=0)
                    # sample for mean
                    invC=np.linalg.inv(self.gibbs_cov[j,i-1])
                    Vn=np.linalg.inv(invV0+n_count[j]*invC)
                    mn=np.dot(Vn,invV0m0+n_count[j]*np.dot(invC,x_mean))
                    prev_mn[j]=mn
                    prev_Vn[j]=Vn
                    self.gibbs_mean[j,i]=np.random.multivariate_normal(mn,Vn)
                    # sample from cov
                    k=np.random.uniform(0,self.max_k)
                    n0=k*n_count[j]
                    S0=n0*S0aux
                    v0=n0+self.p+1
                    vn=v0+n_count[j]
                    St=np.dot((x_-self.gibbs_mean[j,i]).T,(x_-self.gibbs_mean[j,i]))
                    Sn=S0+St
                    prev_vn[j]=vn
                    prev_Sn[j]=Sn
                    self.gibbs_cov[j,i]=invwishart.rvs(df=vn,scale=Sn)
            # sample pi
            self.gibbs_pi[:,i]=np.random.dirichlet(n_count+alpha0/self.n_states)
            for j in range(self.n_states):
                cov_inv=np.linalg.inv(self.gibbs_cov[j,i])
                cov_det=np.linalg.det(self.gibbs_cov[j,i])
                aux[:,j]=self.gibbs_pi[j,i]*mvgauss_prob(y,self.gibbs_mean[j,i],cov_inv,cov_det)   
            # this is a hack to sample fast from a multinomial with different probabilities!
            aux/=np.sum(aux,axis=1)[:,None]
            uni=np.random.uniform(0, 1,size=n)
            aux=np.cumsum(aux,axis=1)
            wrows,wcols=np.where(aux>uni[:,None])
            un,un_idx=np.unique(wrows,return_index=True)
            c=wcols[un_idx]
        self.gibbs_mean=self.gibbs_mean[:,-self.n_gibbs:,:]
        self.gibbs_cov=self.gibbs_cov[:,-self.n_gibbs:,:,:]
        self.gibbs_pi=self.gibbs_pi[:,-self.n_gibbs:]
        self.states_mean=np.mean(self.gibbs_mean,axis=1)
        self.states_cov=np.mean(self.gibbs_cov,axis=1)
        self.states_pi=np.mean(self.gibbs_pi,axis=1)

# code to generate observations from GMM
def simulate_mvgmm(n,phi,means,covs):
    z=np.random.choice(np.arange(phi.size,dtype=int),p=phi,size=n)
    x=np.zeros((n,means[0].size))
    for i in range(n):
        x[i]=np.random.multivariate_normal(means[z[i]],covs[z[i]])
    return x,z

# PARAMETERS
phi=np.array([0.8,0.2])
means=[
        np.array([0.1]),
        np.array([-0.1])
    ] 
covs=[
    np.array([[0.1]]),
    np.array([[0.5]])
]
# number of data points to generate
n=1000 
samples,samples_z=simulate_mvgmm(n,phi,means,covs)        

gmm=GaussianMixture(n_gibbs=2000)
gmm.estimate(samples)
gmm.view(True)
```

    ** GIBBS SAMPLER (GMM) **
    STATE 1
    PROB:  0.8846368773419646
    MEAN
    [0.10202241]
    COVARIANCE
    [[0.12764877]]
    
    


    
![png](/images/gibbs/output_7_1.png)
    



    
![png](/images/gibbs/output_7_2.png)
    


    STATE 2
    PROB:  0.11536312265803549
    MEAN
    [-0.58836726]
    COVARIANCE
    [[0.44062228]]
    
    


    
![png](/images/gibbs/output_7_4.png)
    



    
![png](/images/gibbs/output_7_5.png)
    


## Hidden Markov Model (Gaussian Emissions)

Now we can go to the point of this post, which is to make a Gibbs sampler to estimate a HMM model.

The data $D$ is a sequence of observations $x_0,x_1,\cdots,x_n$; in a similar way to the Gaussian Mixture, the Hidden Markov Model also assumes that the observations can be generated by diferent states but the occurence of the next state depends of the previous one. This means that the observations are not independent and the algorithms need to take this into account. The parameters of the model are the transition matrix $A$, the initial state distribution $P$ and the emission parameters $\mu,C$; in the same fashion of what was done for the Gaussian Mixture, we can consider as variable the sequence of hidden states $z$.

The joint distribution is

$p(D,z,A,P,\mu,C)=p(D\|z,A,P,\mu,C)p(z\|A,P,\mu,C)p(A,P,\mu,C)$

Given that the sequence $z$ is known, data sequence $D$ is independent of $A$ and $P$; also, $z$ is independent of the emission parameters $\mu,C$ (the reasoning here is quite similar to the Gaussian Mixture but now the next emiting state depends on it's previous observations).

A model for the joint can then be written as 

$p(D,z,A,P,\mu,C)=p(D\|z,\mu,C)p(z\|A,P)p(A)p(P)p(\mu)p(C)$

The prior $p(A)$ is a Dirichlet for each row of the transition matrix (this must be easy to see) and for the initial state distribution $p(P)$ the same distribution is adequate (to be conjugate of course).

Using the same idea that the conditional are proportial to the joint, we can look at each parameter individually and keep the terms of our joint model that depend on it.

For the transition matrix $A$:

$p(A\|D,z,P,\mu,C) \propto p(z\|A,P)p(A)$

Then, given the sequence of hidden states $z$, we can write $p(A_{ij}\|D,z,P,\mu,C)=Dir(\alpha_{ij}+I(i\rightarrow j))$ ($I(i\rightarrow j)$ is the number of times the sequence $z$ went from state $i$ to state $j$ - which makes intuitive sense). In a similar way, the the initial state distribution, has a similar form but with the first state in the $z$ sequence (this parameter is more relevant if we are estimating with multiple sequences and so for the majority of the cases we are not that interested in it).

Now, the emission parameters have a form that is equal to the Gaussian Mixture case: given that we have the hidden state variable $z$ the posterior distribution is exactly the same as the one in the previous section (this should be easy to see and to confirm).

Now we still need to sample from the hidden state sequence. Due to the probabilistic structure of the model, $z$ observations are not independent and this needs to be taken into account when calculating the posterior. Let us start by writting: 

$p(z\|D,\theta)=p(z_1,z_2,\cdots,z_n\|D,\theta)=p(z_1\|z_2,z_3,\cdots,z_n\|D,\theta)p(z_2\|z_3,z_4,\cdots,z_n\|D,\theta) \cdots p(z_n\|D,\theta)=p(z_n\|D,\theta)p(z_{n-1}\|z_n,D,\theta)\cdots p(z_1\|z_{1:n},\theta)$

where $\theta$ is just the other parameters (kept fixed now) for ease of notation. This can be simplified as:

$p(z\|D,\theta)=p(z_n\|D,\theta)\Pi_{t=1}^{n-1}p(z_t\|D,z_{t+1:n})$

Looking at the term inside the product:

$p(z_t\|D,z_{t+1:n}) = p(z_t\|x_{1:t},x_{t+1:n},z_{t+1},z_{t+2:n})$

which, given the model probabilistic structure (just consider what depends on what) simplifies to

$p(z_t\|D,z_{t+1:n}) = p(z_t\|x_{1:t},z_{t+1})$

We can note easily the following proportionality relation from the previous equation:

$p(z_t\|x_{1:t},z_{t+1}) \propto p(z_t,x_{1:t},z_{t+1}) \propto p(z_{t+1}\|z_t)p(z_t\|x_{1:t})$

The first term is just the transition matrix; the second term is a familiar term from the forward pass of the HMM: it can be calculated recursively as:

$p(z_t\|x_{1:t}) \propto p(x_t,z_t,x_{1:t+1}) = p(x_t\|z_t)\sum_{z_{t-1}}p(z_t\|z_{t-1})p(z_{t-1}\|x_{1:t-1})$

or, using the $\alpha$ notation:

$\alpha(z_t) = p(x_t\|z_t)\sum_{z_{t-1}}p(z_t\|z_{t-1})\alpha(z_{t-1})$

And so, to generate a sample from the sequence $z$, first we run the forward algorithm to compute $\alpha$ and then we pass backwards, generating a sample for $z_t$ at each step.

##### Multisequence

If we have multiple sequences to learn from, the previous formulas remain the same but we should sample the sequence of $z$ variables for each sequence individually; all the rest remain the same.

##### Higher order and other special cases

To sample from a higher order HMM or to introduce transition constraints we can set some entries of the transition matrix to zero and they will stay zero; also we can set some states to share the same emission (recall that a higher order HMM can be estimated as a regular one but with more states and some of them having the same emission).


The following code implements the previous formulas.



```python
# this can be compiled
@jit(nopython=True)
def forward(prob,A,P):
    '''
    Forward algorithm for the HMM
    prob: numpy (n,n_states) array with
        the probability of each observation
        for each state
    A: numpy (n_states,n_states) array with the state
        transition matrix
    P: numpy (n_states,) array with the initial
        state probability
    returns:
        alpha: numpy (n,n_states) array meaning
            p(state=i\|obs <= i)
        c: numpy (n,) array with the normalization
            constants
    '''
    n_obs=prob.shape[0]
    n_states=prob.shape[1]
    alpha=np.zeros((n_obs,n_states),dtype=np.float64)
    c=np.zeros(n_obs,dtype=np.float64)
    alpha[0]=P*prob[0]
    c[0]=1/np.sum(alpha[0])
    alpha[0]*=c[0]
    for i in range(1,n_obs):
        alpha[i]=np.dot(A.T,alpha[i-1])*prob[i] 
        c[i]=1/np.sum(alpha[i])
        alpha[i]*=c[i]
    return alpha,c

# this can be compiled
@jit(nopython=True)
def backward_sample(A,alpha,q,transition_counter,init_state_counter):
    '''
    Backward sample from the state transition matrix and state sequence
    A: numpy (n_states,n_states) array with the state
        transition matrix
    alpha: numpy (n,n_states) array meaning
        p(state=i\|obs <= i)		
    q: numpy (n,) to store the sample of state sequence
    transition_counter: numpy (n_states,n_states) array to store 
        transition counts to be used to sample a state transition 
        matrix
    init_state_counter: numpy (n_states,) array to store the
        number of times state i is the initial one
    returns:
        none (q and transition_counter are changed inside this function)
    '''	
    # backward walk to sample from the state sequence
    n=q.size
    # sample the last hidden state with probability alpha[-1]
    q[n-1]=np.searchsorted(np.cumsum(alpha[-1]),np.random.random(),side="right")
    # aux variable
    p=np.zeros(A.shape[0],dtype=np.float64)
    # iterate backwards
    for j in range(n-2,-1,-1):
        # from formula
        p=A[:,q[j+1]]*alpha[j] 
        # normalize (from formula)
        p/=np.sum(p) 
        # sample hidden state with probability p
        # this is a trick because jit cannot compile np.random.choice
        q[j]=np.searchsorted(np.cumsum(p),np.random.random(),side="right")
        # increment transition counter (we can do this calculation incrementally)
        transition_counter[q[j],q[j+1]]+=1 
    # increment initial state counter
    init_state_counter[q[0]]+=1

# Gaussian HMM
class GaussianHMM(object):
    def __init__(self,
                 n_states=2,
                 n_gibbs=1000,
                 A_zeros=[],
                 A_groups=[],
                 f_burn=0.1,
                 max_k=0.25,
                 ):
        '''
        n_states: integer with the number of states
        n_gibbs: integer with the number of gibbs iterations
        A_zeros: list of list like [[0,0],[0,1],[3,1]]
            with the entries of the transition matrix that are 
            set to zero
        A_groups: list of lists like [[0,1],[2,3]] of disjoint elements
            where each sublist is the set of states that have the same 
            emissions, i.e, they are the same state
        f_burn: float in (0,1) with the fraction of points to burn at
            the beginning of the samples
        max_k: covariance shrinkage parameter
        '''
        self.n_states=n_states
        self.f_burn=f_burn
        self.n_gibbs=n_gibbs
        self.A_zeros=A_zeros
        self.A_groups=A_groups
        if len(self.A_groups)==0:
            self.A_groups=[[e] for e in range(self.n_states)]   
        self.eff_n_states=len(self.A_groups)
        self.max_k=max_k
        # real number of samples to simulate
        self.n_gibbs_sim=int(self.n_gibbs*(1+self.f_burn))
        self.p=1
        self.P=None
        self.gibbs_P=None
        self.gibbs_A=None
        self.gibbs_mean=None
        self.gibbs_cov=None
        self.A=None
        self.states_mean=None
        self.states_cov=None 
        self.pred_l=None

    def next_state_prob(self,y,l=None):
        '''
        computes a vector with the next state probability
        given a input sequence
        xyq: numpy (n,self.p) array with observation
        l: integer to filter recent data in y -> y=y[-l:]
        '''
        assert y.ndim==2,"y must be a matrix"
        # just return the initial state probability 
        if y.shape[0]==0:
            return self.P
        assert y.shape[1]==self.p,"y must have the same number of variables as the training data"
        if l is not None:
            y=y[-l:]
        if self.states_cov_inv is None:
            self.states_cov_inv=np.zeros((self.eff_n_states,self.p,self.p))
            self.states_cov_det=np.zeros(self.eff_n_states)
            for s in range(self.eff_n_states):
                self.states_cov_inv[s]=np.linalg.inv(self.states_cov[s])
                self.states_cov_det[s]=np.linalg.det(self.states_cov[s])
        n=y.shape[0]
        # declare arrays
        # probability of observations given state
        prob=np.zeros((n,self.n_states),dtype=np.float64) 
        # probability of observations given state
        eff_prob=np.zeros((n,self.eff_n_states),dtype=np.float64) 
        for s in range(self.eff_n_states):
            # use vectorized function
            eff_prob[:,s]=mvgauss_prob(
                                        y,
                                        self.states_mean[s],
                                        self.states_cov_inv[s],
                                        self.states_cov_det[s]
                                        )
            prob[:,self.A_groups[s]]=eff_prob[:,[s]]  
        alpha,_=forward(prob,self.A,self.P)
        next_state_prob=np.dot(self.A.T,alpha[-1])  
        return next_state_prob     
    
    def predict(self,y,**kwargs):
        '''
        Make a prediction for the next 
        observation distribution
        '''
        next_state_prob=self.next_state_prob(y,self.pred_l)
        # group next state prob
        tmp=np.zeros(self.eff_n_states)
        for i,e in enumerate(self.A_groups):
            tmp[i]=np.sum(next_state_prob[e])
        next_state_prob=tmp
        # compute expected value
        mu=np.sum(self.states_mean*next_state_prob[:,None],axis=0)
        # compute second central moment of the mixture distribution
        cov=np.zeros((self.p,self.p))
        for s in range(self.eff_n_states):
            cov+=(next_state_prob[s]*self.states_cov[s])
            cov+=(next_state_prob[s]*self.states_mean[s]*self.states_mean[s][:,None])
        cov-=(mu*mu[:,None])      
        return mu,cov
            
    def view(self,plot_hist=False):
        '''
        plot_hist: if true, plot histograms, otherwise just show the parameters
        '''
        print('** GIBBS SAMPLER (HMM) **')
        print('GROUPS')
        for e in self.A_groups:
            print('- States %s have the same emission'%','.join([str(a) for a in e]))
        print('INITIAL STATE PROBABILITY')
        print(self.P)
        if plot_hist:
            for i in range(self.n_states):
                plt.hist(self.gibbs_P[:,i],density=True,alpha=0.5,label='P[%s]'%(i))
            plt.legend()
            plt.grid(True)
            plt.show()
        print('STATE TRANSITION MATRIX')
        print(self.A)
        print()
        if plot_hist:
            for i in range(self.n_states):
                for j in range(self.n_states):
                    if [i,j] not in self.A_zeros:
                        plt.hist(self.gibbs_A[:,i,j],density=True,alpha=0.5,label='A[%s->%s]'%(i,j))
            plt.legend()
            plt.grid(True)
            plt.show()
        for j in range(self.eff_n_states):
            print('STATE %s'%(j+1))
            print('MEAN')
            print(self.states_mean[j])
            print('COVARIANCE')
            print(self.states_cov[j])
            print()
            if plot_hist:
                if self.gibbs_mean is not None:
                    for i in range(self.p):
                        plt.hist(self.gibbs_mean[j,:,i],density=True,alpha=0.5,label='Mean x%s'%(i+1))
                    plt.legend()
                    plt.grid(True)
                    plt.show()
                if self.gibbs_cov is not None:
                    for i in range(self.p):
                        for q in range(i,self.p):
                            plt.hist(self.gibbs_cov[j,:,i,q],density=True,alpha=0.5,label='Cov(x%s,x%s)'%(i+1,q+1))
                    plt.legend()
                    plt.grid(True)
                    plt.show()

    def estimate(self,y,idx=None,**kwargs):	 
        '''
        Estimate the HMM parameters with Gibbs sampling
        y: numpy (n,p) array
            each row is a joint observation of the variables
        idx: None or array with the indexes that define subsequences
            for example, idx=[[0,5],[5,12],[12,30]] means that subsequence 1 is y[0:5],
            subsequence 2 is y[5:12], subsequence 3 is y[12:30], ... 
        '''
        assert y.ndim==2,"y must be a matrix"
        
        if idx is None:
            idx=np.array([[0,y.shape[0]]],dtype=int)

        # just for safety
        idx=np.array(idx,dtype=int)

        n_seqs=idx.shape[0]

        self.states_cov_inv=None
        self.states_cov_det=None

        n=y.shape[0]
        self.p=y.shape[1]

        # generate variable with the possible states
        states=np.arange(self.n_states,dtype=np.int32)

        # compute data covariance
        c=np.cov(y.T)
        # fix when y has only one column
        if c.ndim==0:
            c=np.array([[c]])
        c_diag=np.diag(np.diag(c)) # diagonal matrix with the covariances

        # Prior distribution parameters
        # these parameters make sense for the type of problems
        # we are trying to solve - assuming zero correlation makes sense
        # as a prior and zero means as well due to the low 
        # values of financial returns
        m0=np.zeros(self.p) # mean: prior location (just put it at zero...)
        V0=c_diag.copy() # mean: prior covariance
        S0aux=c_diag.copy() # covariance prior scale (to be multiplied later)
        alpha0=self.n_states
        alpha=1 # multinomial prior (dirichelet alpha)
        zero_alpha=0.001 # multinomial prior (dirichelet alpha) when there is no transition
        alpha_p=0.05 # multinomial prior (dirichelet alpha) for init state distribution

        # Precalculations
        # the prior alphas need to be calculated before
        # because there may be zero entries in the A matrix
        alphas=[]
        for s in range(self.n_states):
            tmp=alpha*np.ones(self.n_states)
            for e in self.A_zeros:
                if e[0]==s:
                    tmp[e[1]]=zero_alpha
            alphas.append(tmp)

        invV0=np.linalg.inv(V0)
        invV0m0=np.dot(invV0,m0)

        self.eff_n_states=len(self.A_groups)

        # initialize containers
        transition_counter=np.zeros(
                                    (self.n_states,self.n_states)
                                    ) # counter for state transitions
        init_state_counter=np.zeros(
                                    self.n_states
                                    ) # counter for initial state observations
        eff_prob=np.zeros(
                            (n,self.eff_n_states)
                            ) # probability of observations given state
        prob=np.zeros(
                    (n,self.n_states)
                    ) # probability of observations given state

        forward_alpha=np.zeros((n,self.n_states),dtype=np.float64)
        forward_c=np.zeros(n,dtype=np.float64)


        self.gibbs_cov=np.zeros(
                                (self.eff_n_states,self.n_gibbs_sim,self.p,self.p)
                                ) # store sampled covariances
        self.gibbs_mean=np.zeros(
                                (self.eff_n_states,self.n_gibbs_sim,self.p)
                                ) # store sampled means
        self.gibbs_A=np.zeros(
                            (self.n_gibbs_sim,self.n_states,self.n_states)
                            ) # store sampled transition matricess
        self.gibbs_P=np.zeros((self.n_gibbs_sim,self.n_states))

        # initialize covariances and means
        for s in range(self.eff_n_states):
            self.gibbs_mean[s,0]=m0
            self.gibbs_cov[s,0]=c   
        # initialize state transition
        # assume some persistency of state as a initial parameter
        # this makes sense because if this is not the case then this is
        # not very usefull
        if len(self.A_zeros)==0:
            init_mass=0.9
            tmp=init_mass*np.eye(self.n_states)
            remaining_mass=(1-init_mass)/(self.n_states-1)
            tmp[tmp==0]=remaining_mass
            self.gibbs_A[0]=tmp
        else:
            # initialize in a different way!
            tmp=np.ones((self.n_states,self.n_states))
            for e in self.A_zeros:
                tmp[e[0],e[1]]=0
            tmp/=np.sum(tmp,axis=1)[:,None]
            self.gibbs_A[0]=tmp

        self.gibbs_P[0]=np.ones(self.n_states)
        self.gibbs_P[0]/=np.sum(self.gibbs_P[0])

        # create and initialize variable with
        # the states associated with each variable
        # assume equal probability in states
        q=np.random.choice(states,size=n)

        # previous used parameters to sample when there are not observations
        # on that state
        prev_mn=np.zeros((self.n_states,self.p))
        prev_Vn=np.zeros((self.n_states,self.p,self.p))
        prev_vn=np.zeros(self.n_states)
        prev_Sn=np.zeros((self.n_states,self.p,self.p))
        for j in range(self.n_states):
            prev_mn[j]=m0
            prev_Vn[j]=V0
            prev_vn[j]=self.p+1+1
            prev_Sn[j]=S0aux

        # Gibbs sampler
        for i in range(1,self.n_gibbs_sim):
            transition_counter*=0 # set this to zero
            init_state_counter*=0 # set this to zero
            # evaluate the probability of each
            # observation in y under the previously 
            # sampled parameters
            for s in range(self.eff_n_states):
                # compute inverse and determinant
                cov_inv=np.linalg.inv(self.gibbs_cov[s,i-1])
                cov_det=np.linalg.det(self.gibbs_cov[s,i-1])
                # use vectorized function
                eff_prob[:,s]=mvgauss_prob(y,self.gibbs_mean[s,i-1],cov_inv,cov_det)  
                prob[:,self.A_groups[s]]=eff_prob[:,[s]]

            # use multiple sequences
            for l in range(n_seqs):
                # compute alpha variable
                forward_alpha,_=forward(
                                        prob[idx[l][0]:idx[l][1]],
                                        self.gibbs_A[i-1],
                                        self.gibbs_P[i-1]
                                        )
                # backward walk to sample from the state sequence
                backward_sample(
                                self.gibbs_A[i-1],
                                forward_alpha,
                                q[idx[l][0]:idx[l][1]],
                                transition_counter,
                                init_state_counter
                                )
            # now, with a sample from the states (in q variable)
            # it is all quite similar to a gaussian mixture!
            for j in range(self.n_states):
                # sample from transition matrix
                self.gibbs_A[i,j]=np.random.dirichlet(alphas[j]+transition_counter[j])
            # make sure that the entries are zero!
            for e in self.A_zeros:
                self.gibbs_A[i,e[0],e[1]]=0.
            self.gibbs_A[i]/=np.sum(self.gibbs_A[i],axis=1)[:,None]

            # sample from initial state distribution
            self.gibbs_P[i]=np.random.dirichlet(alpha_p+init_state_counter)   

            for j in range(self.eff_n_states):
                # basically, this is the code to sample from a multivariate
                # gaussian but constrained to observations where state=j
                idx_states=np.where(np.in1d(q,self.A_groups[j]))[0]
                # just sample from the prior!
                if idx_states.size==0:
                    self.gibbs_mean[j,i]=np.random.multivariate_normal(prev_mn[j],prev_Vn[j])
                    self.gibbs_cov[j,i]=invwishart.rvs(df=prev_vn[j],scale=prev_Sn[j])
                else:
                    n_count=idx_states.size
                    x_=y[idx_states]
                    y_mean_=np.mean(x_,axis=0)
                    # sample for mean
                    invC=np.linalg.inv(self.gibbs_cov[j,i-1])
                    Vn=np.linalg.inv(invV0+n_count*invC)
                    mn=np.dot(Vn,invV0m0+n_count*np.dot(invC,y_mean_))
                    prev_mn[j]=mn
                    prev_Vn[j]=Vn
                    self.gibbs_mean[j,i]=np.random.multivariate_normal(mn,Vn)
                    # sample from cov
                    # get random k value (shrinkage value)
                    k=np.random.uniform(0,self.max_k)
                    n0=k*n_count
                    S0=n0*S0aux
                    v0=n0+self.p+1
                    vn=v0+n_count
                    St=np.dot((x_-self.gibbs_mean[j,i]).T,(x_-self.gibbs_mean[j,i]))
                    Sn=S0+St
                    prev_vn[j]=vn
                    prev_Sn[j]=Sn
                    self.gibbs_cov[j,i]=invwishart.rvs(df=vn,scale=Sn)
        # burn observations
        self.gibbs_A=self.gibbs_A[-self.n_gibbs:]
        self.gibbs_P=self.gibbs_P[-self.n_gibbs:]
        self.gibbs_mean=self.gibbs_mean[:,-self.n_gibbs:,:]
        self.gibbs_cov=self.gibbs_cov[:,-self.n_gibbs:,:,:]
        self.A=np.mean(self.gibbs_A,axis=0)
        self.P=np.mean(self.gibbs_P,axis=0)
        self.states_mean=np.mean(self.gibbs_mean,axis=1)
        self.states_cov=np.mean(self.gibbs_cov,axis=1)

def simulate_hmm(n,A,P,means,covs):
    states=np.arange(A.shape[0],dtype=int)
    z=np.zeros(n,dtype=int)
    x=np.zeros((n,means[0].size))
    z[0]=np.random.choice(states,p=P)
    x[0]=np.random.multivariate_normal(means[z[0]],covs[z[0]])
    for i in range(1,n):
        z[i]=np.random.choice(states,p=A[z[i-1]])
        x[i]=np.random.multivariate_normal(means[z[i]],covs[z[i]])
    return x,z

# PARAMETERS
A=np.array([
            [0.9,0.1],
            [0.1,0.9]
        ]) # state transition

P=np.array([0.5,0.5]) # initial state distribution

means=[
        np.array([0.1]),
        np.array([-0.1])
    ] # let the means be different from zero 

# list of covariance matrices (for each mixture)
covs=[
    np.array([[0.1]]),
    np.array([[0.5]])
]
# number of data points to generate
n=2000 
samples,samples_z=simulate_hmm(n,A,P,means,covs)
ghmm=GaussianHMM(n_gibbs=1000)
ghmm.estimate(samples)
ghmm.view(True)
```

    ** GIBBS SAMPLER (HMM) **
    GROUPS
    - States 0 have the same emission
    - States 1 have the same emission
    INITIAL STATE PROBABILITY
    [0.70311816 0.29688184]
    


    
![png](/images/gibbs/output_9_1.png)
    


    STATE TRANSITION MATRIX
    [[0.8465251  0.1534749 ]
     [0.09773353 0.90226647]]
    
    


    
![png](/images/gibbs/output_9_3.png)
    


    STATE 1
    MEAN
    [-0.17385274]
    COVARIANCE
    [[0.54737131]]
    
    


    
![png](/images/gibbs/output_9_5.png)
    



    
![png](/images/gibbs/output_9_6.png)
    


    STATE 2
    MEAN
    [0.0926942]
    COVARIANCE
    [[0.1552719]]
    
    


    
![png](/images/gibbs/output_9_8.png)
    



    
![png](/images/gibbs/output_9_9.png)
    



Quite similar to the true parameters.

### Example with a real time series

As an example (of course cherry-picked to look nice), let us try to model Nasdaq futures daily returns with a HMM with gaussian emissions. We will consider two states meaning that we are trying to model the sequence of fluctuations as alternating between positive and negative mean (kind of a trend following strategy).



```python
import pandas as pd
nq1=pd.read_csv('data_nq1.csv',index_col='DATE',parse_dates=True)
nq1.plot(grid=True)
plt.show()
# build returns
nq1=nq1.pct_change().dropna()
```


    
![png](/images/gibbs/output_11_0.png)
    


To start let us just fit the model to the whole data.


```python
# fit model
ghmm_nq1=GaussianHMM(n_gibbs=1000)
ghmm_nq1.estimate(nq1.values)
ghmm_nq1.view(True)
```

    ** GIBBS SAMPLER (HMM) **
    GROUPS
    - States 0 have the same emission
    - States 1 have the same emission
    INITIAL STATE PROBABILITY
    [0.9238397 0.0761603]
    


    
![png](/images/gibbs/output_13_1.png)
    


    STATE TRANSITION MATRIX
    [[0.99440862 0.00559138]
     [0.01490767 0.98509233]]
    
    


    
![png](/images/gibbs/output_13_3.png)
    


    STATE 1
    MEAN
    [0.00097333]
    COVARIANCE
    [[0.00012164]]
    
    


    
![png](/images/gibbs/output_13_5.png)
    



    
![png](/images/gibbs/output_13_6.png)
    


    STATE 2
    MEAN
    [-0.0010826]
    COVARIANCE
    [[0.00075854]]
    
    


    
![png](/images/gibbs/output_13_8.png)
    



    
![png](/images/gibbs/output_13_9.png)
    


We see some evidence of a state with positive mean and other with a negative one. They tend to be persistent. Also, if we look at the posterior of the mean in the negative mean state we observe that there is some probability that this mean is in not negative (although a small value).

This is not usefull to design a strategy because we would like to have some sort of out-of-sample estimation. To achieve this we will train the model with the first half of the data and evaluate a strategy in the other one. It was not discussed previously but the code contains a method to make a prediction of the next distribution given the observations so far. The calculations should be easy to understand: given a sequence we calculate the next state probability with the forward pass and then combine the emissions with this weighting to compute the next period expected return and covariance. Let us do this.


```python
# train model with the first half of the observations
x=nq1.values
n=x.shape[0]
n_train=int(0.5*n)
model=GaussianHMM(n_gibbs=1000)
model.estimate(x[:n_train])
model.view(True)
```

    ** GIBBS SAMPLER (HMM) **
    GROUPS
    - States 0 have the same emission
    - States 1 have the same emission
    INITIAL STATE PROBABILITY
    [0.07082613 0.92917387]
    


    
![png](/images/gibbs/output_15_1.png)
    


    STATE TRANSITION MATRIX
    [[0.99186576 0.00813424]
     [0.00418101 0.99581899]]
    
    


    
![png](/images/gibbs/output_15_3.png)
    


    STATE 1
    MEAN
    [-0.00115037]
    COVARIANCE
    [[0.00095642]]
    
    


    
![png](/images/gibbs/output_15_5.png)
    



    
![png](/images/gibbs/output_15_6.png)
    


    STATE 2
    MEAN
    [0.00069924]
    COVARIANCE
    [[0.00016414]]
    
    


    
![png](/images/gibbs/output_15_8.png)
    



    
![png](/images/gibbs/output_15_9.png)
    


It is a good sign that the found model with just a subset of the data is similar to the one got with all observations. Now let us run a simple walk forward backtest using the predict method; note that we bet with Kelly.


```python
# evaluate the model
s=np.zeros(n)
w=np.zeros(x.shape[1])
for i in tqdm.tqdm(range(n)):
    s[i]=np.dot(w,x[i])
    m,c=model.predict(x[:i+1])
    w=np.dot(np.linalg.inv(c),m)
```

    100%\|█████████████████████████████████████████████████████████████████████████████\| 6234/6234 [00:15<00:00, 395.51it/s]
    


```python
print()
print('Sharpe ratio Strategy [ALL DATA]: ', np.round(np.sqrt(260)*np.mean(s)/np.std(s),3))
print('Sharpe ratio NQ1 [ALL DATA]: ', np.round(np.sqrt(260)*np.mean(x[:,0])/np.std(x[:,0]),3))
print()
print('Sharpe ratio Strategy [OOS]: ', np.round(np.sqrt(260)*np.mean(s[n_train:])/np.std(s[n_train:]),3))
print('Sharpe ratio NQ1 [OOS]: ', np.round(np.sqrt(260)*np.mean(x[:,0][n_train:])/np.std(x[:,0][n_train:]),3))
plt.plot(np.cumsum(s/np.std(s)),label='Strategy [Normalized]')
plt.plot(np.cumsum(x[:,0]/np.std(x[:,0])),label='NQ1 [Normalized]')
plt.grid(True)
plt.axvline(n_train)
plt.show()
```

    
    Sharpe ratio Strategy [ALL DATA]:  0.762
    Sharpe ratio NQ1 [ALL DATA]:  0.39
    
    Sharpe ratio Strategy [OOS]:  0.926
    Sharpe ratio NQ1 [OOS]:  0.895
    


    
![png](/images/gibbs/output_18_1.png)
    


We got an interesting results (the vertical line separate out-of-sample from training data) although this is not a feasible strategy because our weights need to be bounded. Also, one could estimate the HMM parameters with the last half of the data and evaluate the strategy on the first half (if we have similar model parameters on this dataset then the performance should be similar). 

All of this could be done but the point here was just to illustrate the use of the sampler to make conclusion about infered parameters from data. 


```python

```

